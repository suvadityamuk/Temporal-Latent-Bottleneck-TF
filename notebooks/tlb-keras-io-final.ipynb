{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3-0DzUGk-mCY"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "A simple Recurrent Neural Network (RNN) displays a strong inductive bias towards learning **temporally compressed representations**. **Equation 1** shows the recurrence formula, where `h_t` is the compressed representation (a single vector) of the entire input sequence `x`.\n",
        "\n",
        "| ![Equation of RNN](https://i.imgur.com/Kdyj2jr.png) |\n",
        "| :--: |\n",
        "| **Equation 1**: The recurrence equation. (Source: Aritra and Suvaditya)|\n",
        "\n",
        "On the other hand, Transformers ([Vaswani et. al](https://arxiv.org/abs/1706.03762)) have little inductive bias towards learning temporally compressed representations. It has achieved SoTA results in Natural Language Processing (NLP) and Vision tasks with its pair-wise attention mechanism.\n",
        "\n",
        "While the Transformer has the ability to **attend** to different sections of the input sequence, the computation of attention is quadratic in nature.\n",
        "\n",
        "[Didolkar et. al](https://arxiv.org/abs/2205.14794) argue that having a more compressed representation of a sequence may be beneficial for *generalization*, as it can be easily **re-used** and **re-purposed** with fewer irrelevant details. While compression is good, they also notice that too much of it can harm expressiveness.\n",
        "\n",
        "The authors propose a solution that divides computation into **two streams**. A *slow stream* that is recurrent in nature and a *fast stream* that is parameterized as a Transformer. While this method has the novelty of introducing different processing streams in order to preserve and process latent states, it has parallels drawn in other works like the [Perceiver Mechanism (by Jaegle et. al.)](https://arxiv.org/abs/2103.03206) and [Grounded Language Learning Fast and Slow (by Hill et. al.)](https://arxiv.org/pdf/2009.01719.pdf).\n",
        "\n",
        "The following example explores how we can make use of the new Temporal Latent Bottleneck mechanism to perform image classification on the CIFAR-10 dataset. We implement this model by making a custom `RNNCell` implementation in order to make a **performant** and **vectorized** design.\n",
        "\n",
        "_Note_: This example makes use of `TensorFlow 2.11.0`, which must be installed into our system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4WMiqn0DhOJ"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UShqLtBqBos7"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow==2.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0Kp-qS_-jp9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras.optimizers.experimental import AdamW\n",
        "\n",
        "import random\n",
        "from typing import Tuple, List\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Set seed for reproducibility.\n",
        "tf.keras.utils.set_random_seed(42)\n",
        "\n",
        "AUTO = tf.data.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgzyibaJJoSc"
      },
      "source": [
        "## Setting required configuration\n",
        "\n",
        "We set a few configuration parameters that are needed within the pipeline we have designed. The current parameters are for use with the [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
        "\n",
        "\n",
        "The model also supports `mixed-precision` settings, which would quantize the model to use `16-bit` float numbers where it can, while keeping some parameters in `32-bit` as needed for numerical stability. This brings performance benefits as the footprint of the model decreases significantly while bringing speed boosts at inference-time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiaIYOEaJmqY"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"mixed_precision\": True,\n",
        "    \"dataset\": \"cifar10\",\n",
        "    \"train_slice\": 40_000,\n",
        "    \"batch_size\": 1024,\n",
        "    \"buffer_size\": 1024 * 2,\n",
        "    \"input_shape\": [32, 32, 3],\n",
        "    \"image_size\": 48,\n",
        "    \"num_classes\": 10,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"epochs\": 50,\n",
        "    \"patch_size\": 4,\n",
        "    \"embed_dim\": 128,\n",
        "    \"chunk_size\": 8,\n",
        "    \"r\": 2,\n",
        "    \"num_layers\": 6,\n",
        "    \"ffn_drop\": 0.2,\n",
        "    \"attn_drop\": 0.2,\n",
        "    \"num_heads\": 1,\n",
        "}\n",
        "\n",
        "if config[\"mixed_precision\"]:\n",
        "    policy = mixed_precision.Policy(\"mixed_float16\")\n",
        "    mixed_precision.set_global_policy(policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQzLELJMLiby"
      },
      "source": [
        "## Loading the CIFAR-10 dataset\n",
        "\n",
        "We are going to use the CIFAR10 dataset for running our experiments. This dataset contains a training set of `50,000` images for `10` classes with the standard image size of `(32, 32, 3)`.\n",
        "\n",
        "It also has a separate set of `10,000` images with similar characteristics. More information about the dataset may be found at the official site for the dataset as well as [`keras.datasets.cifar10`](https://keras.io/api/datasets/cifar10/) API reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56I-6BY8Lrge"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "(x_train, y_train), (x_val, y_val) = (\n",
        "    (x_train[: config[\"train_slice\"]], y_train[: config[\"train_slice\"]]),\n",
        "    (x_train[config[\"train_slice\"] :], y_train[config[\"train_slice\"] :]),\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vF4GM51uLuQS"
      },
      "source": [
        "## Define Augmentation pipelines for Train and Validation/Test pipelines\n",
        "\n",
        "We define separate pipelines for performing image augmentation on our data. This step is important in pre-processing the data, making the model more robust to changes, helping it to generalize better. The steps we perform are as follows:\n",
        "\n",
        "- `Rescaling` (Training, Test): This step is performed to normalize all image pixel values from the [0,255] range to [0,1). This helps in maintaining numerical stability later ahead during training.\n",
        "\n",
        "- `Resizing` (Training, Test): We resize the image from it's original size of (32, 32) to (52, 52). This is done to account for the Random Crop, as well as comply with the specifications of the data given in the paper.\n",
        "\n",
        "- `RandomCrop` (Training): This layer will randomly select a crop/sub-region of the image with size (48, 48).\n",
        "\n",
        "- `RandomFlip` (Training): This layer will randomly flip all the images horizontally, keeping sizes same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73JF9xvfLudv"
      },
      "outputs": [],
      "source": [
        "# Build the `train` augmentation pipeline.\n",
        "train_aug = keras.Sequential(\n",
        "    [\n",
        "        layers.Rescaling(1 / 255.0, dtype=\"float32\"),\n",
        "        layers.Resizing(\n",
        "            config[\"input_shape\"][0] + 20,\n",
        "            config[\"input_shape\"][0] + 20,\n",
        "            dtype=\"float32\",\n",
        "        ),\n",
        "        layers.RandomCrop(config[\"image_size\"], config[\"image_size\"], dtype=\"float32\"),\n",
        "        layers.RandomFlip(\"horizontal\", dtype=\"float32\"),\n",
        "    ],\n",
        "    name=\"train_data_augmentation\",\n",
        ")\n",
        "\n",
        "# Build the `val` and `test` data pipeline.\n",
        "test_aug = keras.Sequential(\n",
        "    [\n",
        "        layers.Rescaling(1 / 255.0, dtype=\"float32\"),\n",
        "        layers.Resizing(config[\"image_size\"], config[\"image_size\"], dtype=\"float32\"),\n",
        "    ],\n",
        "    name=\"test_data_augmentation\",\n",
        ")\n",
        "\n",
        "# We define functions in place of simple lambda functions to run through the\n",
        "# `keras.Sequential`in order to solve this warning:\n",
        "# (https://github.com/tensorflow/tensorflow/issues/56089)\n",
        "\n",
        "def train_map_fn(image, label):\n",
        "    return train_aug(image), label\n",
        "\n",
        "def test_map_fn(image, label):\n",
        "    return test_aug(image), label"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zwhDkZoXV_Xg"
      },
      "source": [
        "## Load dataset into `tf.data.Dataset` object\n",
        "\n",
        "- We take the `np.ndarray` instance of the datasets and move them into a `tf.data.Dataset` instance\n",
        "- Apply augmentations using [`.map()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map)\n",
        "- Shuffle the dataset using [`.shuffle()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle)\n",
        "- Batch the dataset using [`.batch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch)\n",
        "- Enable pre-fetching of batches using [`.prefetch()`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VA4XqGL-L4AP"
      },
      "outputs": [],
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_ds = (\n",
        "    train_ds.map(train_map_fn, num_parallel_calls=AUTO)\n",
        "    .shuffle(config[\"buffer_size\"])\n",
        "    .batch(config[\"batch_size\"], num_parallel_calls=AUTO)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_ds = (\n",
        "    val_ds.map(test_map_fn, num_parallel_calls=AUTO)\n",
        "    .batch(config[\"batch_size\"], num_parallel_calls=AUTO)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_ds = (\n",
        "    test_ds.map(test_map_fn, num_parallel_calls=AUTO)\n",
        "    .batch(config[\"batch_size\"], num_parallel_calls=AUTO)\n",
        "    .prefetch(AUTO)\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Awq70Il0oy-2"
      },
      "source": [
        "## Temporal Latent Bottleneck\n",
        "\n",
        "An excerpt from the paper:\n",
        "\n",
        "> In the brain, short-term and long-term memory have developed in a specialized way. Short-term memory is allowed to change very quickly to react to immediate sensory inputs and perception. By contrast, long-term memory changes slowly, is highly selective and involves repeated consolidation.\n",
        "\n",
        "Inspired from the short-term and long-term memory the authors introduce the fast stream and slow stream computation. The fast stream has a short-term memory with a high capacity that reacts quickly to sensory input (Transformers). The slow stream has long-term memory which updates at a slower rate and summarizes the most relevant information (Recurrence).\n",
        "\n",
        "To implement this idea we need to:\n",
        "\n",
        "- Take a sequence of data.\n",
        "- Divide the sequence into fixed-size chunks.\n",
        "- Fast stream operates within each chunk. It provides fine-grained local information.\n",
        "- Slow stream consolidates and aggregates information across chunks. It provides coarse-grained distant information.\n",
        "\n",
        "The fast and slow stream induce what is called **information asymmetry**. The two streams interact with each other through a bottleneck of attention. **Figure 1** shows the architecture of the model.\n",
        "\n",
        "| ![Architecture of the model](https://i.imgur.com/bxdLPNH.png) |\n",
        "| :--: |\n",
        "| Figure 1: Architecture of the model. (Source: https://arxiv.org/abs/2205.14794) |\n",
        "\n",
        "A PyTorch-style pseudocode is also proposed by the authors as shown in **Algorithm 1**.\n",
        "\n",
        "| ![Pseudocode of the model](https://i.imgur.com/s8a5Vz9.png) |\n",
        "| :--: |\n",
        "| Algorithm 1: PyTorch styled pseudocode. (Source: https://arxiv.org/abs/2205.14794) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_59-HEbML0m"
      },
      "source": [
        "### `PatchEmbed` layer\n",
        "\n",
        "This custom `keras.layers.Layer` is useful for generating patches from the image and transform them into a higher-dimensional embedding space using `keras.layers.Embedding`. The patching operation is done using a `keras.layers.Conv2D` instance instead of a traditional `tf.image.extract_patches` to allow for vectorization.  \n",
        "\n",
        "Once the patching of images is complete, we reshape the image patches in order to get a flattened representation where the number of dimensions is the embedding dimension. At this stage, we also inject positional information to the tokens.\n",
        "\n",
        "After we obtain the tokens we chunk them. The chunking operation involves taking fixed-size sequences from the embedding output to create 'chunks', which will then be used as the final input to the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGCVL-j_ML_c"
      },
      "outputs": [],
      "source": [
        "class PatchEmbed(layers.Layer):\n",
        "    \"\"\"Image to Patch Embedding.\n",
        "    Args:\n",
        "        image_size (`Tuple[int]`): Size of the input image.\n",
        "        patch_size (`Tuple[int]`): Size of the patch.\n",
        "        embed_dim (`int`): Dimension of the embedding.\n",
        "        chunk_size (`int`): Number of patches to be chunked.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size: Tuple[int],\n",
        "        patch_size: Tuple[int],\n",
        "        embed_dim: int,\n",
        "        chunk_size: int,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Compute the patch resolution.\n",
        "        patch_resolution = [\n",
        "            image_size[0] // patch_size[0],\n",
        "            image_size[1] // patch_size[1],\n",
        "        ]\n",
        "\n",
        "        # Store the parameters.\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.patch_resolution = patch_resolution\n",
        "        self.num_patches = patch_resolution[0] * patch_resolution[1]\n",
        "\n",
        "        # Define the positions of the patches.\n",
        "        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "\n",
        "        # Create the layers.\n",
        "        self.projection = layers.Conv2D(\n",
        "            filters=embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            strides=patch_size,\n",
        "            name=\"projection\",\n",
        "        )\n",
        "        self.flatten = layers.Reshape(\n",
        "            target_shape=(-1, embed_dim),\n",
        "            name=\"flatten\",\n",
        "        )\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=self.num_patches,\n",
        "            output_dim=embed_dim,\n",
        "            name=\"position_embedding\",\n",
        "        )\n",
        "        self.layernorm = keras.layers.LayerNormalization(\n",
        "            epsilon=1e-5,\n",
        "            name=\"layernorm\",\n",
        "        )\n",
        "        self.chunking_layer = layers.Reshape(\n",
        "            target_shape=(self.num_patches // chunk_size, chunk_size, embed_dim),\n",
        "            name=\"chunking_layer\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs: tf.Tensor) -> Tuple[tf.Tensor, int, int, int]:\n",
        "        \"\"\"Call function.\n",
        "        Args:\n",
        "            inputs (`tf.Tensor`): Input tensor.\n",
        "        Returns:\n",
        "            `Tuple[tf.Tensor, int, int, int]`: Tuple of the projected input, number of patches,\n",
        "                patch resolution, and embedding dimension.\n",
        "        \"\"\"\n",
        "        # Project the inputs to the embedding dimension.\n",
        "        x = self.projection(inputs)\n",
        "\n",
        "        # Flatten the pathces and add position embedding.\n",
        "        x = self.flatten(x)\n",
        "        x = x + self.position_embedding(self.positions)\n",
        "\n",
        "        # Normalize the embeddings.\n",
        "        x = self.layernorm(x)\n",
        "\n",
        "        # Chunk the tokens.\n",
        "        x = self.chunking_layer(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9bzxrEXMVOx"
      },
      "source": [
        "### `FeedForwardNetwork` Layer\n",
        "\n",
        "This custom `keras.layers.Layer` instance allows us to define a generic FFN along with a dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqIx2OYNMcN-"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNetwork(layers.Layer):\n",
        "    \"\"\"Feed Forward Network.\n",
        "    Args:\n",
        "        dims (`int`): Number of units in FFN.\n",
        "        dropout (`float`): Dropout probability for FFN.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dims: int, dropout: float, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Create the layers.\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=4 * dims, activation=tf.nn.gelu),\n",
        "                layers.Dense(units=dims),\n",
        "                layers.Dropout(rate=dropout),\n",
        "            ],\n",
        "            name=\"ffn\",\n",
        "        )\n",
        "        self.add = layers.Add(\n",
        "            name=\"add\",\n",
        "        )\n",
        "        self.layernorm = layers.LayerNormalization(\n",
        "            epsilon=1e-5,\n",
        "            name=\"layernorm\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"Call function.\n",
        "        Args:\n",
        "            inputs (`tf.Tensor`): Input tensor.\n",
        "        Returns:\n",
        "            `tf.Tensor`: Output tensor.\"\"\"\n",
        "        # Apply the FFN.\n",
        "        x = self.layernorm(inputs)\n",
        "        x = self.add([inputs, self.ffn(x)])\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF1sYwhpMhHV"
      },
      "source": [
        "### `BaseAttention` layer\n",
        "\n",
        "This custom `keras.layers.Layer` instance is a `super`/`base` class that wraps a `keras.layers.MultiHeadAttention` layer along with some other components. This gives us basic common denominator functionality for all the Attention layers/modules in our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhXH_cmCMhP7"
      },
      "outputs": [],
      "source": [
        "class BaseAttention(layers.Layer):\n",
        "    \"\"\"Base Attention Module.\n",
        "    Args:\n",
        "        num_heads (`int`): Number of attention heads.\n",
        "        key_dim (`int`): Size of each attention head for key.\n",
        "        dropout (`float`): Dropout probability for attention module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads: int, key_dim: int, dropout: float, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.mha = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=key_dim,\n",
        "            dropout=dropout,\n",
        "            name=\"mha\",\n",
        "        )\n",
        "        self.q_layernorm = layers.LayerNormalization(\n",
        "            epsilon=1e-5,\n",
        "            name=\"q_layernorm\",\n",
        "        )\n",
        "        self.k_layernorm = layers.LayerNormalization(\n",
        "            epsilon=1e-5,\n",
        "            name=\"k_layernorm\",\n",
        "        )\n",
        "        self.v_layernorm = layers.LayerNormalization(\n",
        "            epsilon=1e-5,\n",
        "            name=\"v_layernorm\",\n",
        "        )\n",
        "        self.add = layers.Add(\n",
        "            name=\"add\",\n",
        "        )\n",
        "\n",
        "        self.attn_scores = None\n",
        "\n",
        "    def call(\n",
        "        self, input_query: tf.Tensor, key: tf.Tensor, value: tf.Tensor\n",
        "    ) -> tf.Tensor:\n",
        "        \"\"\"Call function.\n",
        "        Args:\n",
        "            input_query (`tf.Tensor`): Input query tensor.\n",
        "            key (`tf.Tensor`): Key tensor.\n",
        "            value (`tf.Tensor`): Value tensor.\n",
        "        Returns:\n",
        "            `tf.Tensor`: Output tensor.\"\"\"\n",
        "        # Apply the attention module.\n",
        "        query = self.q_layernorm(input_query)\n",
        "        key = self.k_layernorm(key)\n",
        "        value = self.v_layernorm(value)\n",
        "        (attn_outs, attn_scores) = self.mha(\n",
        "            query=query,\n",
        "            key=key,\n",
        "            value=value,\n",
        "            return_attention_scores=True,\n",
        "        )\n",
        "\n",
        "        # Save the attention scores for later visualization.\n",
        "        self.attn_scores = attn_scores\n",
        "\n",
        "        # Add the input to the attention output.\n",
        "        x = self.add([input_query, attn_outs])\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVCOm7XpMraC"
      },
      "source": [
        "### `Attention` with `FeedForwardNetwork` layer\n",
        "\n",
        "This custom `keras.layers.Layer` implementation combines the `BaseAttention` and `FeedForwardNetwork` components to develop one block which will be used repeatedly within the model. This module is highly customizable and flexible, allowing for changes within the internal layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MslECoZyMqv_"
      },
      "outputs": [],
      "source": [
        "class AttentionWithFFN(layers.Layer):\n",
        "    \"\"\"Attention with Feed Forward Network.\n",
        "    Args:\n",
        "        ffn_dims (`int`): Number of units in FFN.\n",
        "        ffn_dropout (`float`): Dropout probability for FFN.\n",
        "        num_heads (`int`): Number of attention heads.\n",
        "        key_dim (`int`): Size of each attention head for key.\n",
        "        attn_dropout (`float`): Dropout probability for attention module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        ffn_dims: int,\n",
        "        ffn_dropout: float,\n",
        "        num_heads: int,\n",
        "        key_dim: int,\n",
        "        attn_dropout: float,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        # Create the layers.\n",
        "        self.attention = BaseAttention(\n",
        "            num_heads=num_heads,\n",
        "            key_dim=key_dim,\n",
        "            dropout=attn_dropout,\n",
        "            name=\"base_attn\",\n",
        "        )\n",
        "        self.ffn = FeedForwardNetwork(\n",
        "            dims=ffn_dims,\n",
        "            dropout=ffn_dropout,\n",
        "            name=\"ffn\",\n",
        "        )\n",
        "\n",
        "        self.attn_scores = None\n",
        "\n",
        "    def call(self, query: tf.Tensor, key: tf.Tensor, value: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"Call function.\n",
        "        Args:\n",
        "            query (`tf.Tensor`): Input query tensor.\n",
        "            key (`tf.Tensor`): Key tensor.\n",
        "            value (`tf.Tensor`): Value tensor.\n",
        "        Returns:\n",
        "            `tf.Tensor`: Output tensor.\n",
        "        \"\"\"\n",
        "        # Apply the attention module.\n",
        "        x = self.attention(query, key, value)\n",
        "\n",
        "        # Save the attention scores for later visualization.\n",
        "        self.attn_scores = self.attention.attn_scores\n",
        "\n",
        "        # Apply the FFN.\n",
        "        x = self.ffn(x)\n",
        "        return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XtgsSqYPMyd-"
      },
      "source": [
        "### Custom RNN Cell for **Temporal Latent Bottleneck** and **Perceptual Module**\n",
        "\n",
        "**Algorithm 1** (the pseudocode) depicts recurrence with the help of for loops. Looping does make the implementation simpler, harming the training time. In this section we wrap the custom recurrence logic inside of the `CustomRecurrentCell`. This custom cell will then be wrapped with the [Keras RNN API](https://keras.io/api/layers/recurrent_layers/rnn/) that makes the entire code vectorizable.\n",
        "\n",
        "This custom cell, implemented as a `keras.layers.Layer`, is the integral part of the logic for the model.\n",
        "The cell's functionality can be divided into 2 parts:\n",
        "- **Slow Stream (Temporal Latent Bottleneck):**\n",
        "\n",
        "    - This module consists of a single `AttentionWithFFN` layer that parses the output of the previous Slow Stream, an intermediate hidden representation (which is the *latent* in Temporal Latent Bottleneck) as the Query, and the output of the latest Fast Stream as Key and Value. This layer can also be construed as a *CrossAttention* layer.\n",
        "\n",
        "- **Fast Stream (Perceptual Module):**\n",
        "    \n",
        "    - This module consists of intertwined `AttentionWithFFN` layers. This stream consists of *n* layers of `SelfAttention` and `CrossAttention` in a sequential manner. \n",
        "    - Here, some layers take the chunked input as the Query, Key and Value (Also referred to as the *SelfAttention* layer). \n",
        "    - The other layers take the intermediate state outputs from within the Temporal Latent Bottleneck module as the Query while using the output of the previous Self-Attention layers before it as the Key and Value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7dHOLunMyv7"
      },
      "outputs": [],
      "source": [
        "class CustomRecurrentCell(layers.Layer):\n",
        "    \"\"\"Custom Recurrent Cell.\n",
        "    Args:\n",
        "        chunk_size (`int`): Number of tokens in a chunk.\n",
        "        r (`int`): One Cross Attention per **r** Self Attention.\n",
        "        num_layers (`int`): Number of layers.\n",
        "        ffn_dims (`int`): Number of units in FFN.\n",
        "        ffn_dropout (`float`): Dropout probability for FFN.\n",
        "        num_heads (`int`): Number of attention heads.\n",
        "        key_dim (`int`): Size of each attention head for key.\n",
        "        attn_dropout (`float`): Dropout probability for attention module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        chunk_size: int,\n",
        "        r: int,\n",
        "        num_layers: int,\n",
        "        ffn_dims: int,\n",
        "        ffn_dropout: float,\n",
        "        num_heads: int,\n",
        "        key_dim: int,\n",
        "        attn_dropout: float,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        # Save the arguments.\n",
        "        self.chunk_size = chunk_size\n",
        "        self.r = r\n",
        "        self.num_layers = num_layers\n",
        "        self.ffn_dims = ffn_dims\n",
        "        self.ffn_droput = ffn_dropout\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.attn_dropout = attn_dropout\n",
        "\n",
        "        # Create the state_size and output_size. This is important for\n",
        "        # custom recurrence logic.\n",
        "        self.state_size = tf.TensorShape([chunk_size, ffn_dims])\n",
        "        self.output_size = tf.TensorShape([chunk_size, ffn_dims])\n",
        "\n",
        "        self.get_attn_scores = False\n",
        "        self.attn_scores = []\n",
        "\n",
        "        ########################################################################\n",
        "        # Perceptual Module\n",
        "        ########################################################################\n",
        "        perceptual_module = list()\n",
        "        for layer_idx in range(num_layers):\n",
        "            perceptual_module.append(\n",
        "                AttentionWithFFN(\n",
        "                    ffn_dims=ffn_dims,\n",
        "                    ffn_dropout=ffn_dropout,\n",
        "                    num_heads=num_heads,\n",
        "                    key_dim=key_dim,\n",
        "                    attn_dropout=attn_dropout,\n",
        "                    name=f\"pm_self_attn_{layer_idx}\",\n",
        "                )\n",
        "            )\n",
        "            if layer_idx % r == 0:\n",
        "                perceptual_module.append(\n",
        "                    AttentionWithFFN(\n",
        "                        ffn_dims=ffn_dims,\n",
        "                        ffn_dropout=ffn_dropout,\n",
        "                        num_heads=num_heads,\n",
        "                        key_dim=key_dim,\n",
        "                        attn_dropout=attn_dropout,\n",
        "                        name=f\"pm_cross_attn_ffn_{layer_idx}\",\n",
        "                    )\n",
        "                )\n",
        "        self.perceptual_module = perceptual_module\n",
        "\n",
        "        ########################################################################\n",
        "        # Temporal Latent Bottleneck Module\n",
        "        ########################################################################\n",
        "        self.tlb_module = AttentionWithFFN(\n",
        "            ffn_dims=ffn_dims,\n",
        "            ffn_dropout=ffn_dropout,\n",
        "            num_heads=num_heads,\n",
        "            key_dim=key_dim,\n",
        "            attn_dropout=attn_dropout,\n",
        "            name=f\"tlb_cross_attn_ffn\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, states) -> Tuple[tf.Tensor, List[tf.Tensor]]:\n",
        "        \"\"\"Call function.\n",
        "        Args:\n",
        "            inputs (`tf.Tensor`): Input tensor.\n",
        "            states (`List[tf.Tensor]`): List of state tensors.\n",
        "        Returns:\n",
        "            `Tuple[tf.Tensor, List[tf.Tensor]]`: Tuple of output tensor and list\n",
        "                of state tensors.\n",
        "        \"\"\"\n",
        "        # inputs => (batch, chunk_size, dims)\n",
        "        # states => [(batch, chunk_size, units)]\n",
        "        slow_stream = states[0]\n",
        "        fast_stream = inputs\n",
        "\n",
        "        for layer_idx, layer in enumerate(self.perceptual_module):\n",
        "            fast_stream = layer(query=fast_stream, key=fast_stream, value=fast_stream)\n",
        "\n",
        "            if layer_idx % self.r == 0:\n",
        "                fast_stream = layer(\n",
        "                    query=fast_stream, key=slow_stream, value=slow_stream\n",
        "                )\n",
        "\n",
        "        slow_stream = self.tlb_module(\n",
        "            query=slow_stream, key=fast_stream, value=fast_stream\n",
        "        )\n",
        "\n",
        "        # Save the attention scores for later visualization.\n",
        "        if self.get_attn_scores:\n",
        "            self.attn_scores.append(self.tlb_module.attn_scores)\n",
        "\n",
        "        return fast_stream, [slow_stream]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQBcEAhHNNq0"
      },
      "source": [
        "### `ModelTrainer` to encapsulate full model\n",
        "\n",
        "Here, we just wrap the full model as to expose it for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00MBGlysNN6e"
      },
      "outputs": [],
      "source": [
        "class ModelTrainer(keras.Model):\n",
        "    \"\"\"Model Trainer.\n",
        "    Args:\n",
        "        patch_layer (`tf.keras.layers.Layer`): Patching layer.\n",
        "        custom_cell (`tf.keras.layers.Layer`): Custom Recurrent Cell.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, patch_layer, custom_cell, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.patch_layer = patch_layer\n",
        "        self.rnn = layers.RNN(custom_cell, name=\"rnn\")\n",
        "        self.gap = layers.GlobalAveragePooling1D(name=\"gap\")\n",
        "        self.head = layers.Dense(10, activation=\"softmax\", dtype=\"float32\", name=\"head\")\n",
        "\n",
        "    def call(self, inputs: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"Call function.\n",
        "        Args:\n",
        "            inputs (`tf.Tensor`): Input tensor.\n",
        "        Returns:\n",
        "            `tf.Tensor`: Output tensor.\n",
        "        \"\"\"\n",
        "        x = self.patch_layer(inputs)\n",
        "        x = self.rnn(x)\n",
        "        x = self.gap(x)\n",
        "        outputs = self.head(x)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h4vDmObNY9n"
      },
      "source": [
        "## Build the model\n",
        "\n",
        "To begin training, we now define the components individually and pass them as arguments to our wrapper class, which will prepare the final model for training. We define a `PatchEmbed` layer, and the `CustomCell`-based RNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWwOgSzONV6P"
      },
      "outputs": [],
      "source": [
        "# We call this to clear all previous session states. This frees up GPU memory\n",
        "# consumption as well.\n",
        "\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# Build the model.\n",
        "patch_layer = PatchEmbed(\n",
        "    image_size=(config[\"image_size\"], config[\"image_size\"]),\n",
        "    patch_size=(config[\"patch_size\"], config[\"patch_size\"]),\n",
        "    embed_dim=config[\"embed_dim\"],\n",
        "    chunk_size=config[\"chunk_size\"],\n",
        ")\n",
        "custom_rnn_cell = CustomRecurrentCell(\n",
        "    chunk_size=config[\"chunk_size\"],\n",
        "    r=config[\"r\"],\n",
        "    num_layers=config[\"num_layers\"],\n",
        "    ffn_dims=config[\"embed_dim\"],\n",
        "    ffn_dropout=config[\"ffn_drop\"],\n",
        "    num_heads=config[\"num_heads\"],\n",
        "    key_dim=config[\"embed_dim\"],\n",
        "    attn_dropout=config[\"attn_drop\"]\n",
        ")\n",
        "model = ModelTrainer(patch_layer=patch_layer, custom_cell=custom_rnn_cell)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MaEpxiD4Pdt3"
      },
      "source": [
        "## Metrics and Callbacks\n",
        "\n",
        "We use the `AdamW` optimizer from `tf.keras.optimizers.experimental` (previously part of `tensorflow-addons`) since it has been shown to perform very well on several benchmark tasks from an optimization perspective. It is a version of the `tf.keras.optimizers.Adam` optimizer, along with Weight Decay in place.\n",
        "\n",
        "For a loss function, we make use of the `keras.losses.SparseCategoricalCrossentropy` function that makes use of simple Cross-entropy between prediction and actual logits. We also calculate accuracy on our data as a sanity-check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K45_j8LYPdLh"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamW(\n",
        "    learning_rate=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"]\n",
        ")\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=keras.optimizers.SparseCategoricalCrossentropy,\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YymGB8NjQfsG"
      },
      "source": [
        "## Train the model with `model.fit()`\n",
        "\n",
        "We pass the training dataset and run training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE5WJ616PuKO"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=config[\"epochs\"],\n",
        "    validation_data=val_ds,\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KF0mpzl6Qjsm"
      },
      "source": [
        "## Visualize training metrics\n",
        "\n",
        "The `model.fit()` will return a `history` object, which stores the values of the metrics generated during the training run (but it is ephemeral and needs to be saved manually).\n",
        "\n",
        "We now display the Loss and Accuracy curves for the training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74tjVkybQj2K"
      },
      "outputs": [],
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history[\"accuracy\"], label=\"accuracy\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gm2Bs3UpQ2yL"
      },
      "source": [
        "## Visualize attention maps from the Temporal Latent Bottleneck\n",
        "\n",
        "Now that we have trained our model it is time for some visualizations. The Fast Stream (Transformers) processes a chunk of tokens. The Slow Stream processes each chunk and attends to tokens that are useful for the task.\n",
        "\n",
        "In this section we will visualize the attention map of the Slow Stream. This is done by extracting the attention scores from the TLB layer at each chunk's intersection and storing it within the RNN's state. This is followed by 'ballooning' it up and returning these values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV1YSMDIQyJ0"
      },
      "outputs": [],
      "source": [
        "def score_to_viz(chunk_score):\n",
        "    # get the most attended token\n",
        "    chunk_viz = tf.math.reduce_max(chunk_score, axis=-2)\n",
        "    # get the mean across heads\n",
        "    chunk_viz = tf.math.reduce_mean(chunk_viz, axis=1)  \n",
        "    return chunk_viz\n",
        "\n",
        "# Get a batch of images and labels from the testing dataset\n",
        "images, labels = next(iter(test_ds))\n",
        "\n",
        "# Set the get_attn_scores flag to True\n",
        "model.rnn.cell.get_attn_scores = True\n",
        "\n",
        "# Run the model with the testing images and grab the\n",
        "# attention scores.\n",
        "outputs = model(images)\n",
        "list_chunk_scores = model.rnn.cell.attn_scores\n",
        "\n",
        "# Process the attention scores in order to visualize them\n",
        "list_chunk_viz = [score_to_viz(x) for x in list_chunk_scores]\n",
        "chunk_viz = tf.concat(list_chunk_viz[1:], axis=-1)\n",
        "chunk_viz = tf.reshape(\n",
        "    chunk_viz,\n",
        "    (\n",
        "        config[\"batch_size\"],\n",
        "        config[\"image_size\"] // config[\"patch_size\"],\n",
        "        config[\"image_size\"] // config[\"patch_size\"],\n",
        "        1,\n",
        "    ),\n",
        ")\n",
        "upsampled_heat_map = layers.UpSampling2D(\n",
        "    size=(4, 4), interpolation=\"bilinear\", dtype=\"float32\"\n",
        ")(chunk_viz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNm3aMQE1ziQ"
      },
      "source": [
        "Run the following code snippet to get different images and their attention maps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4rNfJLsRDOw"
      },
      "outputs": [],
      "source": [
        "# Sample a random image\n",
        "index = random.randint(0, config[\"batch_size\"])\n",
        "orig_image = images[index]\n",
        "overlay_image = upsampled_heat_map[index, ..., 0]\n",
        "\n",
        "# Plot the visualization\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
        "\n",
        "ax[0].imshow(orig_image)\n",
        "ax[0].set_title(\"Original:\")\n",
        "ax[0].axis(\"off\")\n",
        "\n",
        "image = ax[1].imshow(orig_image)\n",
        "ax[1].imshow(\n",
        "    overlay_image,\n",
        "    cmap=\"inferno\",\n",
        "    alpha=0.6,\n",
        "    extent=image.get_extent(),\n",
        ")\n",
        "ax[1].set_title(\"TLB Attention:\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZtyth9pMdHe"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This example has hereby demonstrated an implementation of the Temporal Latent Bottleneck mechanism. The example highlights the use of compression and storage of historical states in the form of a Temporal Latent Bottleneck with regular updates from a Perceptual Module as an effective method to do so.\n",
        "\n",
        "In the original paper, the authors have conducted highly extensive tests around different modalities ranging from Supervised Image Classification to applications in Reinforcement Learning.  \n",
        "\n",
        "While we have only displayed a method to apply this mechanism to Image Classification, it can be extended to other modalities too with minimal changes.\n",
        "\n",
        "*Note*: While building this example we did not have the official code to refer to. This means that our implementation is inspired from the paper with no claims of being a complete reproduction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4xRJKbC3Fks"
      },
      "source": [
        "## Acknowledgement\n",
        "\n",
        "We would like to thank [PyImageSearch](https://pyimagesearch.com/) for a Colab Pro account and [JarvisLabs.ai](https://cloud.jarvislabs.ai/) for GPU credits."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "5b3ded1ccb95c1d9bd405e7b823d9e85424cde40fbb5985eb47e999ef50e15b4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
