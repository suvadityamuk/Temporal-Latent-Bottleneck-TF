{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Name: Exploring Temporal Latent Bottlenecks for Image Classification\n",
    "\n",
    "**Author**: Aritra Roy Gosthipaty, Suvaditya Mukherjee  \n",
    "**Date Created:** 06/03/2023  \n",
    "**Last Modified:** 07/03/2023  \n",
    "**Description:** Performing Image Classification with State-of-the-art Temporal Latent Bottleneck Mechanism.  \n",
    "**Accelerator:** GPU"
   ],
   "metadata": {
    "id": "T5aX5cf8CCwm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "The following example explores how we can make use of the new Temporal Latent Bottleneck mechanism to perform image classification on the CIFAR-10 dataset. We implement this model by making a custom `RNNCell` implementation in order to make a performant and vectorized design, as proposed by [Didolkar et. al](https://arxiv.org/abs/2205.14794)  \n",
    "A simple Recurrent Neural Network displays strong [inductive bias](https://en.wikipedia.org/wiki/Inductive_bias), i.e. the ability to generalize well within a specific domain. But it faces the significant problem of Vanishing/Exploding Gradients, along with the inability to store hidden-state information for long sequences.  \n",
    "On the other end of the spectrum, the concept of the [Attention-based Transformer mechanism as introduced by Vaswani et. al](https://arxiv.org/abs/1706.03762) has shown considerable improvements in those departments, wherein it has achieved State-of-the-art results in Natural Language Processing tasks while also being adapted and used considerably in the Vision domain. While the Transformer has the ability to \\\"attend\\\" to different sections of the input sequence, it suffers from lacking inductive bias. This makes the mechanism prone to not generalizing well to domain-specific tasks.  \n",
    "\"This paper combines the concepts from both ends of the spectrum in order to make a new mechanism which has the ability to tackle the problem of inductive biases, vanishing/exploding gradient and loss of information with higher sequence lengths. While this method has the novelty of introducing different processing streams in order to preserve and process latent states, it has parallels drawn in other works like the [Perceiver Mechanism (by Jaegle et. al.)](https://arxiv.org/abs/2103.03206) and [Grounded Language Learning Fast and Slow (by Hill et. al.)](https://arxiv.org/pdf/2009.01719.pdf).  \n",
    "\n",
    "This example is structured as follows:\n",
    "- Perform necessary imports\n",
    "- Set-up required configurations and settings\n",
    "- Load the [CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "- Visualize random samples from the dataset\n",
    "- Define Base layer for Attention and `PatchEmbed` layer for performing Patching and Embedding operations\n",
    "- Define the `AttentionWithFFN` layer\n",
    "- Compose the Perceptual Module and Temporal Latent Bottleneck Module as a stack of `SelfAttentionWithFFN` and `CrossAttentionWithFFN` layers\n",
    "- Create custom `RNNCell` implementation which makes use of the above-mentioned modules (vectorized) and load into a Recurrent Neural Network\n",
    "- Define hyperparameters and `model.fit()` pipeline\n",
    "- Perform inference and testing\n",
    "\n",
    "This example makes use of `TensorFlow 2.11.0`, which must be installed into our system"
   ],
   "metadata": {
    "id": "3-0DzUGk-mCY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup imports"
   ],
   "metadata": {
    "id": "d4WMiqn0DhOJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q tensorflow==2.11.0"
   ],
   "metadata": {
    "id": "UShqLtBqBos7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0Kp-qS_-jp9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers.experimental import AdamW\n",
    "from tensorflow.keras import mixed_precision\n",
    "from typing import Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "tf.keras.utils.set_random_seed(42)\n",
    "\n",
    "AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting required configuration\n",
    "\n",
    "We set a few configuration parameters that are needed within the pipeline we have designed. The current parameters are for use with the [CIFAR10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  \n",
    "The model also supports mixed-precision settings, which would quantize the model to use 16-bit float numbers where it can, while keeping some parameters in 32-bit as needed for numerical stability. This brings performance benefits as the footprint of the model decreases significantly while bring speed boosts at inference-time."
   ],
   "metadata": {
    "id": "fgzyibaJJoSc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "config = {\n",
    "    \"mixed_precision\": True,\n",
    "    \"dataset\": \"cifar10\",\n",
    "    \"train_slice\": 40_000,\n",
    "    \"batch_size\": 1024,\n",
    "    \"buffer_size\": 1024 * 2,\n",
    "    \"input_shape\": [32, 32, 3],\n",
    "    \"image_size\": 48,\n",
    "    \"num_classes\": 10,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"epochs\": 100,\n",
    "    \"patch_size\": 4,\n",
    "    \"embed_dim\": 128,\n",
    "    \"chunk_size\": 8,\n",
    "    \"r\": 2,\n",
    "    \"num_layers\": 6,\n",
    "    \"ffn_drop\": 0.2,\n",
    "    \"attn_drop\": 0.2,\n",
    "    \"num_heads\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "policy = mixed_precision.Policy(\"mixed_float16\")\n",
    "mixed_precision.set_global_policy(policy)"
   ],
   "metadata": {
    "id": "RiaIYOEaJmqY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the CIFAR-10 dataset\n",
    "\n",
    "As mentioned previously in the Introduction, we are going to use the CIFAR10 dataset for running our experiments. This dataset contains a training set of 50,000 images for 10 classes with the standard image size of (32, 32, 3). It also has a separate set of 10,000 images with similar characteristics. More information about the dataset may be found at the official site for the dataset as well as [`keras.datasets.cifar10`](https://keras.io/api/datasets/cifar10/) API reference"
   ],
   "metadata": {
    "id": "bQzLELJMLiby"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "(x_train, y_train), (x_val, y_val) = (\n",
    "    (x_train[: config[\"train_slice\"]], y_train[: config[\"train_slice\"]]),\n",
    "    (x_train[config[\"train_slice\"] :], y_train[config[\"train_slice\"] :]),\n",
    ")"
   ],
   "metadata": {
    "id": "56I-6BY8Lrge"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Augmentation pipelines for Train and Validation/Test pipelines\n",
    "\n",
    "We define separate pipelines for performing image augmentation on our data. This step is important in pre-processing the data, making the model more robust to changes, and help it to generalize better. The steps we perform are as follows:\n",
    "\n",
    "- `Rescaling` (Training, Test): This step is performed to normalize all image pixel values from the [0,255] range to [0,1). This helps in maintaining numerical stability later ahead during training.\n",
    "\n",
    "- `Resizing` (Training, Test): We resize the image from it's original size of (32, 32) to (52, 52). This is done to account for the Random Crop, as well as comply with the specifications of the data given in the paper.\n",
    "\n",
    "- `RandomCrop` (Training): This layer will randomly select a crop/sub-region of the image with size (48, 48).\n",
    "\n",
    "- `RandomFlip` (Training): This layer will randomly flip all the images horizontally, keeping sizes same."
   ],
   "metadata": {
    "id": "vF4GM51uLuQS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Build the `train` augmentation pipeline.\n",
    "train_aug = keras.Sequential(\n",
    "    [\n",
    "        layers.Rescaling(1 / 255.0, dtype=\"float32\"),\n",
    "        layers.Resizing(\n",
    "            config[\"input_shape\"][0] + 20,\n",
    "            config[\"input_shape\"][0] + 20,\n",
    "            dtype=\"float32\",\n",
    "        ),\n",
    "        layers.RandomCrop(config[\"image_size\"], config[\"image_size\"], dtype=\"float32\"),\n",
    "        layers.RandomFlip(\"horizontal\", dtype=\"float32\"),\n",
    "    ],\n",
    "    name=\"train_data_augmentation\",\n",
    ")\n",
    "\n",
    "# Build the `val` and `test` data pipeline.\n",
    "test_aug = keras.Sequential(\n",
    "    [\n",
    "        layers.Rescaling(1 / 255.0, dtype=\"float32\"),\n",
    "        layers.Resizing(config[\"image_size\"], config[\"image_size\"], dtype=\"float32\"),\n",
    "    ],\n",
    "    name=\"test_data_augmentation\",\n",
    ")"
   ],
   "metadata": {
    "id": "73JF9xvfLudv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "## We define functions in place of simple lambda functions to run through the\n",
    "## `keras.Sequential`in order to solve this warning:\n",
    "## (https://github.com/tensorflow/tensorflow/issues/56089)\n",
    "\n",
    "\n",
    "def train_map_fn(image, label):\n",
    "    return train_aug(image), label\n",
    "\n",
    "\n",
    "def test_map_fn(image, label):\n",
    "    return test_aug(image), label"
   ],
   "metadata": {
    "id": "bXmPLtnBL7Sp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load dataset into `tf.data.Dataset` and perform optimizations\n",
    "\n",
    "- We take the `np.ndarray` instance of the datasets and move them into a `tf.data.Dataset` instance\n",
    "- Apply augmentations using `.map()`\n",
    "- Shuffle the dataset using `.shuffle()`\n",
    "- Batch the dataset using `.batch()`\n",
    "- Enable pre-fetching of batches using `.prefetch()`"
   ],
   "metadata": {
    "id": "zwhDkZoXV_Xg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = (\n",
    "    train_ds.map(train_map_fn, num_parallel_calls=AUTO)\n",
    "    .shuffle(config[\"buffer_size\"])\n",
    "    .batch(config[\"batch_size\"], num_parallel_calls=AUTO)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_ds = (\n",
    "    val_ds.map(test_map_fn, num_parallel_calls=AUTO)\n",
    "    .batch(config[\"batch_size\"], num_parallel_calls=AUTO)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_ds = (\n",
    "    test_ds.map(test_map_fn, num_parallel_calls=AUTO)\n",
    "    .batch(config[\"batch_size\"], num_parallel_calls=AUTO)\n",
    "    .prefetch(AUTO)\n",
    ")"
   ],
   "metadata": {
    "id": "VA4XqGL-L4AP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate number of batches\n",
    "total_batches_train = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "print(f\"Total batches to train on: {total_batches_train}\")"
   ],
   "metadata": {
    "id": "vG4M9ARGL_a3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Check if the `tf.data.Dataset` instance returns iterative outputs\n",
    "\n",
    "image, label = next(iter(train_ds))\n",
    "print(image.shape)\n",
    "print(label.shape)"
   ],
   "metadata": {
    "id": "G1ulvaYMMCx4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define `PatchEmbed` layer\n",
    "\n",
    "This custom `keras.layers.Layer` instance is useful for generating patches from the image and transform them into a higher-dimensional embedding space using `keras.layers.Embedding`.  \n",
    "The patching operation is done using a `keras.layers.Conv2D` instance instead of a traditional `tf.image.extract_patches` to allow for vectorization to take place.  \n",
    "Once the patching of images is complete, we reshape the image patches in order to get a flattened representation where the number of dimensions is the Embedding dimension. At this stage, we also add a Positional Embedding factor into the input.  \n",
    "We then pass the images into the Embedding layer, following which we go through a `keras.layers.LayerNormalization`, finally performing the 'chunking' operation.  \n",
    "The Chunking operation involves taking fixed-size sequences from the embedding output to create 'chunks', which will then be used as the final input to the model."
   ],
   "metadata": {
    "id": "-_59-HEbML0m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class PatchEmbed(layers.Layer):\n",
    "    \"\"\"Image patch embedding layer.\n",
    "\n",
    "    Args:\n",
    "        image_size (Tuple[int]): Input image resolution.\n",
    "        patch_size (Tuple[int]): Patch spatial resolution.\n",
    "        embed_dim (int): Embedding dimension.\n",
    "        add_pos_info (bool): Whether to add positional information to tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: Tuple[int] = (48, 48),\n",
    "        patch_size: Tuple[int] = (4, 4),\n",
    "        embed_dim: int = 32,\n",
    "        chunk_size: int = 8,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # Get patch resolution\n",
    "        patch_resolution = [\n",
    "            image_size[0] // patch_size[0],\n",
    "            image_size[1] // patch_size[1],\n",
    "        ]\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_resolution = patch_resolution\n",
    "\n",
    "        # Calculate number of patches per image\n",
    "        self.num_patches = patch_resolution[0] * patch_resolution[1]\n",
    "        self.proj = layers.Conv2D(\n",
    "            filters=embed_dim, kernel_size=patch_size, strides=patch_size\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=self.num_patches, output_dim=embed_dim\n",
    "        )\n",
    "\n",
    "        # Calculate number of positions for the patches.\n",
    "        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        self.norm = keras.layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "        # Perform Chunking\n",
    "        self.chunking_layer = layers.Reshape(\n",
    "            target_shape=(self.num_patches // chunk_size, chunk_size, embed_dim)\n",
    "        )\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> Tuple[tf.Tensor, int, int, int]:\n",
    "        \"\"\"Patchifies the image, converts into tokens and adds pos information.\n",
    "\n",
    "        Args:\n",
    "            x: Tensor of shape (B, H, W, C)\n",
    "\n",
    "        Returns:\n",
    "            A tuple of the processed tensor, height of the projected\n",
    "            feature map, width of the projected feature map, number\n",
    "            of channels of the projected feature map.\n",
    "        \"\"\"\n",
    "        # Project the inputs.\n",
    "        x = self.proj(x)\n",
    "        x = self.flatten(x)\n",
    "        x = x + self.position_embedding(self.positions)\n",
    "\n",
    "        # B, H, W, C -> B, H*W, C\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Chunk the tokens in K\n",
    "        x = self.chunking_layer(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "id": "iGCVL-j_ML_c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define `FeedForwardNetwork`\n",
    "\n",
    "This custom `keras.layers.Layer` instance allows us to define a generic FFN along with a dropout."
   ],
   "metadata": {
    "id": "w9bzxrEXMVOx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class FeedForwardNetwork(layers.Layer):\n",
    "    \"\"\"Feed Forward Network.\n",
    "\n",
    "    Args:\n",
    "        dims (`int`): Dimension of the FFN.\n",
    "        dropout (`float`): Dropout probability of FFN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dims: int, dropout: float = 0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=4 * dims, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=dims),\n",
    "                layers.Dropout(rate=dropout),\n",
    "            ]\n",
    "        )\n",
    "        self.add = layers.Add()\n",
    "        self.layernorm = layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor):\n",
    "        x = self.layernorm(inputs)\n",
    "        x = self.add([inputs, self.ffn(x)])\n",
    "        return x"
   ],
   "metadata": {
    "id": "qqIx2OYNMcN-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define `BaseAttention` as base class for Attention modules\n",
    "\n",
    "This custom `keras.layers.Layer` instance is a `super`/`base` class that wraps a `keras.layers.MultiHeadAttention` layer along with some other components. This gives us basic common denominator functionality for all the Attention layers/modules in our model."
   ],
   "metadata": {
    "id": "UF1sYwhpMhHV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class BaseAttention(layers.Layer):\n",
    "    \"\"\"The base attention module.\n",
    "\n",
    "    Args:\n",
    "        num_heads (`int`): Number of attention heads.\n",
    "        key_dim (`int`): Size of each attention head for query and key.\n",
    "        dropout (`float`): Dropout probability for Attention Module.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, key_dim: int, dropout: float = 0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mha = layers.MultiHeadAttention(num_heads, key_dim, dropout=dropout)\n",
    "        self.q_layernorm = layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.k_layernorm = layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.v_layernorm = layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.add = layers.Add()\n",
    "\n",
    "    def call(self, input_query: tf.Tensor, key: tf.Tensor, value: tf.Tensor):\n",
    "        query = self.q_layernorm(input_query)\n",
    "        key = self.k_layernorm(key)\n",
    "        value = self.v_layernorm(value)\n",
    "        (attn_outs, attn_scores) = self.mha(\n",
    "            query=query,\n",
    "            key=key,\n",
    "            value=value,\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "        self.attn_scores = attn_scores\n",
    "        x = self.add([input_query, attn_outs])\n",
    "        return x"
   ],
   "metadata": {
    "id": "jhXH_cmCMhP7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define `Attention` module with `FeedForwardNetwork` at head\n",
    "\n",
    "This custom `keras.layers.Layer` implementation combines the `BaseAttention` and `FeedForwardNetwork` components to develop one block which will be used repeatedly within the model. This module is highly customizable and flexible, allowing for changes within the internal layers."
   ],
   "metadata": {
    "id": "oVCOm7XpMraC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class AttentionWithFFN(layers.Layer):\n",
    "    \"\"\"Self-attention module with FFN\n",
    "\n",
    "    Args:\n",
    "        ffn_dims (`int`): Number of units in FFN.\n",
    "        ffn_dropout (`float`): Dropout probability for FFN.\n",
    "        num_heads (`int`): Number of attention heads.\n",
    "        key_dim (`int`): Size of each attention head for query and key.\n",
    "        attn_dropout (`float`): Dropout probability for attention module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        ffn_dims: int = 128,\n",
    "        ffn_dropout: float = 0.1,\n",
    "        num_heads: int = 4,\n",
    "        key_dim: int = 256,\n",
    "        attn_dropout: float = 0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = BaseAttention(\n",
    "            num_heads, key_dim, attn_dropout, name=\"BaseAttention\"\n",
    "        )\n",
    "        self.ffn = FeedForwardNetwork(ffn_dims, ffn_dropout, name=\"FeedForward\")\n",
    "\n",
    "    def call(self, query: tf.Tensor, key: tf.Tensor, value: tf.Tensor):\n",
    "        x = self.attention(query, key, value)\n",
    "        self.attn_scores = self.attention.attn_scores\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ],
   "metadata": {
    "id": "MslECoZyMqv_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Custom Recurrent Neural Network cell layer for **Temporal Latent Bottleneck** and **Perceptual Module**\n",
    "\n",
    "This custom cell, implemented as a `keras.layers.Layer`, is the integral part of the logic for the model.\n",
    "The cell's functionality can be divided into 2 parts:\n",
    "- **Slow Stream (Temporal Latent Bottleneck):** \n",
    "    - This module consists of a single `AttentionWithFFN` layer that parses the output of the previous Slow Stream, an intermediate hidden representation (which is the *latent* in the Temporal Latent Bottleneck) as the Query, and the output of the latest Fast Stream as Key and Value. This layer can also be construed as a *CrossAttention* layer.  \n",
    "\n",
    "- **Fast Stream (Perceptual Module):** \n",
    "     - This module consists of intertwined `AttentionWithFFN` layers.This stream consists of *n* layers of `SelfAttention` and `CrossAttention` in a sequential manner. \n",
    "     - Here, some layers take the chunked input as the Query, Key and Value (Also referred to as the *SelfAttention* layer). \n",
    "     - The other layers take the intermediate state outputs from within the Temporal Latent Bottleneck module as the Query while using the output of the previous Self-Attention layers before it as the Key and Value. "
   ],
   "metadata": {
    "id": "XtgsSqYPMyd-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class CustomCell(layers.Layer):\n",
    "    \"\"\"Custom logic inside each recurrence.\n",
    "\n",
    "    Args:\n",
    "        chunk_size (`int`): Chunk size of the inputs.\n",
    "        r (`int`): One Cross Attention per **r** Self Attention.\n",
    "        num_layers (`int`): Number of layers in the Perceptual Model.\n",
    "        ffn_dims (`int`): Number of units in FFN.\n",
    "        ffn_dropout (`float`): Dropout probability for FFN.\n",
    "        num_heads (`int`): Number of attention heads.\n",
    "        key_dim (`int`): Size of each attention head for query and key.\n",
    "        attn_dropout (`float`): Dropout probability for attention module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size,\n",
    "        r=2,\n",
    "        num_layers: int = 5,\n",
    "        ffn_dims: int = 128,\n",
    "        ffn_dropout: float = 0.1,\n",
    "        num_heads: int = 4,\n",
    "        key_dim: int = 256,\n",
    "        attn_dropout: float = 0.1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.r = r\n",
    "        self.num_layers = num_layers\n",
    "        self.ffn_dims = ffn_dims\n",
    "        self.ffn_droput = ffn_dropout\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.attn_dropout = attn_dropout\n",
    "\n",
    "        # Required for any custom `RNNCell` implementations\n",
    "        self.state_size = tf.TensorShape([chunk_size, ffn_dims])\n",
    "        self.output_size = tf.TensorShape([chunk_size, ffn_dims])\n",
    "\n",
    "        self.get_attn_scores = False\n",
    "        self.attn_scores = []\n",
    "\n",
    "        ########################################################################\n",
    "        # Perceptual Module\n",
    "        ########################################################################\n",
    "        perceptual_module = list()\n",
    "        for layer_idx in range(num_layers):\n",
    "            perceptual_module.append(\n",
    "                AttentionWithFFN(\n",
    "                    ffn_dims=ffn_dims,\n",
    "                    ffn_dropout=ffn_dropout,\n",
    "                    num_heads=num_heads,\n",
    "                    key_dim=key_dim,\n",
    "                    attn_dropout=attn_dropout,\n",
    "                    name=f\"PM_SelfAttentionFFN{layer_idx}\",\n",
    "                )\n",
    "            )\n",
    "            if layer_idx % r == 0:\n",
    "                perceptual_module.append(\n",
    "                    AttentionWithFFN(\n",
    "                        ffn_dims=ffn_dims,\n",
    "                        ffn_dropout=ffn_dropout,\n",
    "                        num_heads=num_heads,\n",
    "                        key_dim=key_dim,\n",
    "                        attn_dropout=attn_dropout,\n",
    "                        name=f\"PM_CrossAttentionFFN{layer_idx}\",\n",
    "                    )\n",
    "                )\n",
    "        self.perceptual_module = perceptual_module\n",
    "\n",
    "        ########################################################################\n",
    "        # Temporal Latent Bottleneck Module\n",
    "        ########################################################################\n",
    "        self.tlb_module = AttentionWithFFN(\n",
    "            ffn_dims=ffn_dims,\n",
    "            ffn_dropout=ffn_dropout,\n",
    "            num_heads=num_heads,\n",
    "            key_dim=key_dim,\n",
    "            attn_dropout=attn_dropout,\n",
    "            name=f\"TLBM_CrossAttentionFFN\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, states, training=None):\n",
    "        # inputs => (batch, chunk_size, dims)\n",
    "        # states => [(batch, chunk_size, units)]\n",
    "\n",
    "        slow_stream = states[0]\n",
    "        fast_stream = inputs\n",
    "\n",
    "        for layer_idx, layer in enumerate(self.perceptual_module):\n",
    "            fast_stream = layer(query=fast_stream, key=fast_stream, value=fast_stream)\n",
    "\n",
    "            if layer_idx % self.r == 0:\n",
    "                fast_stream = layer(\n",
    "                    query=fast_stream, key=slow_stream, value=slow_stream\n",
    "                )\n",
    "\n",
    "        slow_stream = self.tlb_module(\n",
    "            query=slow_stream, key=fast_stream, value=fast_stream\n",
    "        )\n",
    "\n",
    "        if self.get_attn_scores:\n",
    "            self.attn_scores.append(self.tlb_module.attn_scores)\n",
    "\n",
    "        return fast_stream, [slow_stream]"
   ],
   "metadata": {
    "id": "G7dHOLunMyv7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define `ModelTrainer` to encapsulate full model\n",
    "\n",
    "Here, we just wrap the full model as to expose it for training."
   ],
   "metadata": {
    "id": "KQBcEAhHNNq0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ModelTrainer(keras.Model):\n",
    "    def __init__(self, patch_layer, custom_cell, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.patch_layer = patch_layer\n",
    "        self.rnn = layers.RNN(custom_cell)\n",
    "        self.gap = layers.GlobalAveragePooling1D()\n",
    "        self.head = layers.Dense(10, activation=\"softmax\", dtype=\"float32\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.patch_layer(inputs)\n",
    "        x = self.rnn(x)\n",
    "        x = self.gap(x)\n",
    "        outputs = self.head(x)\n",
    "        return outputs"
   ],
   "metadata": {
    "id": "00MBGlysNN6e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Model Components for creating Training Pipeline\n",
    "\n",
    "To begin training, we now define the components individually and pass them as arguments to our wrapper class, which will prepare the final model for training. We define a `PatchEmbed` layer, and the `CustomCell`-based RNN."
   ],
   "metadata": {
    "id": "9h4vDmObNY9n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# We call this to clear all previous session states. This frees up GPU memory\n",
    "# consumption as well.\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Patch & Embedding\n",
    "patch_layer = PatchEmbed(\n",
    "    image_size=(config[\"image_size\"], config[\"image_size\"]),\n",
    "    patch_size=(config[\"patch_size\"], config[\"patch_size\"]),\n",
    "    embed_dim=config[\"embed_dim\"],\n",
    "    chunk_size=config[\"chunk_size\"],\n",
    ")\n",
    "\n",
    "# Recurrent Cell\n",
    "cell = CustomCell(\n",
    "    chunk_size=config[\"chunk_size\"],\n",
    "    r=config[\"r\"],\n",
    "    num_layers=config[\"num_layers\"],\n",
    "    ffn_dims=config[\"embed_dim\"],\n",
    "    ffn_dropout=config[\"ffn_drop\"],\n",
    "    num_heads=config[\"num_heads\"],\n",
    "    key_dim=config[\"embed_dim\"],\n",
    "    attn_dropout=config[\"attn_drop\"],\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = ModelTrainer(patch_layer, cell)"
   ],
   "metadata": {
    "id": "BWwOgSzONV6P"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compile Model and set-up metrics + callbacks tracking\n",
    "\n",
    "We use the `AdamW` optimizer from `tf.keras.optimizers.experimental` (previously part of `tensorflow-addons`) since it has been shown to perform very well on several benchmark tasks from an optimization perspective. It is a version of the `tf.keras.optimizers.Adam` optimizer, along with Weight Decay in place.  \n",
    "For a loss function, we make use of the `keras.losses.SparseCategoricalCrossentropy` function that makes use of simple Cross-entropy between prediction and actual logits. We also calculate accuracy on our dataset.  \n",
    "We define a callback that will be triggered after each epoch ends, to help track all metrics and state variables with the help of Tensorboard. [Know more about how Tensorboard is useful over here](https://www.tensorflow.org/tensorboard)"
   ],
   "metadata": {
    "id": "MaEpxiD4Pdt3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = AdamW(\n",
    "    learning_rate=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"]\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=keras.optimizers.SparseCategoricalCrossentropy,\n",
    "    metrics=keras.metrics.SparseCategoricalAccuracy,\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir=\"tfboard-logs\",\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_images=True,\n",
    "        update_freq=\"epoch\",\n",
    "        embeddings_freq=10,\n",
    "    )\n",
    "]"
   ],
   "metadata": {
    "id": "K45_j8LYPdLh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the model with `model.fit()`\n",
    "\n",
    "We pass the training dataset and run training for 100 epochs."
   ],
   "metadata": {
    "id": "YymGB8NjQfsG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=config[\"epochs\"],\n",
    "    validation_data=val_ds,\n",
    "    callbacks=callbacks,\n",
    ")"
   ],
   "metadata": {
    "id": "ZE5WJ616PuKO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize training metrics\n",
    "\n",
    "The `model.fit()` will return a History object, which stores the values of the metrics generated during the training run (but it is ephemeral and needs to be stored manually).  \n",
    "We now display the Loss and Accuracy curves for the training and validation sets."
   ],
   "metadata": {
    "id": "KF0mpzl6Qjsm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"accuracy\"], label=\"accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "74tjVkybQj2K"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize attention maps from the Temporal Latent Bottleneck\n",
    "\n",
    "We visualize the attention scores returned from the Temporal Latent Bottleneck. This is done by extracting the attention scores from the TLB Cross-attention layer at each chunk's intersection and storing it within the RNN's state. This is followed by 'ballooning' it up and returning these values. Finally, we process the shape of the tensors and return the scores as a heatmap overlaid on the original image."
   ],
   "metadata": {
    "id": "gm2Bs3UpQ2yL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "images, labels = next(iter(test_ds))\n",
    "\n",
    "# Set the flag for attn score\n",
    "model.rnn.cell.get_attn_scores = True\n",
    "outputs = model(images)\n",
    "\n",
    "# Grab the list of chunk scores\n",
    "list_chunk_scores = model.rnn.cell.attn_scores"
   ],
   "metadata": {
    "id": "jV1YSMDIQyJ0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def score_to_viz(chunk_score):\n",
    "    chunk_viz = tf.math.reduce_max(chunk_score, axis=-2)  # get the most attended token\n",
    "    chunk_viz = tf.math.reduce_mean(chunk_viz, axis=1)  # get the mean across heads\n",
    "    return chunk_viz"
   ],
   "metadata": {
    "id": "P4rNfJLsRDOw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "list_chunk_viz = [score_to_viz(x) for x in list_chunk_scores]"
   ],
   "metadata": {
    "id": "MtstSOziRFQ4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "chunk_viz = tf.concat(list_chunk_viz[1:], axis=-1)\n",
    "chunk_viz = tf.reshape(\n",
    "    chunk_viz,\n",
    "    (\n",
    "        config[\"batch_size\"],\n",
    "        config[\"image_size\"] // config[\"patch_size\"],\n",
    "        config[\"image_size\"] // config[\"patch_size\"],\n",
    "        1,\n",
    "    ),\n",
    ")"
   ],
   "metadata": {
    "id": "3h2usqzfRGh5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "upsampled_heat_map = layers.UpSampling2D(\n",
    "    size=(4, 4), interpolation=\"bilinear\", dtype=\"float32\"\n",
    ")(chunk_viz)"
   ],
   "metadata": {
    "id": "UW2EMHB7RIYi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "index = 60\n",
    "orig_image = images[index]\n",
    "overlay_image = upsampled_heat_map[index, ..., 0]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "ax[0].imshow(orig_image)\n",
    "ax[0].set_title(\"Original:\")\n",
    "ax[0].axis(\"off\")\n",
    "\n",
    "image = ax[1].imshow(orig_image)\n",
    "ax[1].imshow(\n",
    "    overlay_image,\n",
    "    cmap=\"inferno\",\n",
    "    alpha=0.6,\n",
    "    extent=image.get_extent(),\n",
    ")\n",
    "ax[1].set_title(\"TLB Attention:\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "oxFNbmlsRKhH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "This example has hereby demonstrated an implementation of the Temporal Latent Bottleneck mechanism which involves using a mix of Attention mechanisms to bring about a solution to the inductive bias problem of Transformers. The example highlights the use of compression and storage of historical states in the form of a Temporal Latent Bottleneck with regular updates from a Perceptual Module as an effective method to do so. In the original paper, the authors have conducted highly extensive tests around different modalities ranging from Supervised Image Classification to applications in Reinforcement Learning.  \n",
    "<TODO WRITE A PARA ON FINAL TEST/TRAIN METRICS>  \n",
    "While we have only displayed a method to apply this mechanism to Image Classification, it can be extended to other modalities too with minimal changes."
   ],
   "metadata": {
    "id": "gZtyth9pMdHe"
   }
  }
 ]
}