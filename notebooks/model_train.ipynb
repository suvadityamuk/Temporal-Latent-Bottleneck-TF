{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hb66VOLCLHjC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers.experimental import AdamW\n",
    "from typing import Optional, Tuple, List\n",
    "from matplotlib import pyplot as plt\n",
    "from random import randint\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z9AwzrziLVkw",
    "outputId": "eb778beb-2a64-41cc-b261-c9278654d17b"
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJDSbV2bCfOo"
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "TRAIN_SLICE = 40000\n",
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = BATCH_SIZE*2\n",
    "AUTO = tf.data.AUTOTUNE\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "IMAGE_SIZE = 48\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 5e-2\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 25\n",
    "\n",
    "# MODEL\n",
    "PATCH_SIZE = 4\n",
    "EMBED_DIM = 128\n",
    "CHUNK_SIZE = 8\n",
    "R = 2\n",
    "NUM_LAYERS = 5\n",
    "FFN_DROP = 0.1\n",
    "ATTN_DROP = 0.1\n",
    "NUM_HEADS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLSxYNjFCloE",
    "outputId": "a41df220-cedc-4900-dbd6-e5d465883cbf"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "(x_train, y_train), (x_val, y_val) = (\n",
    "    (x_train[:TRAIN_SLICE], y_train[:TRAIN_SLICE]),\n",
    "    (x_train[TRAIN_SLICE:], y_train[TRAIN_SLICE:]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmRBDXphCoDu"
   },
   "outputs": [],
   "source": [
    "# Build the `train` augmentation pipeline.\n",
    "train_aug = keras.Sequential(\n",
    "    [\n",
    "        layers.Rescaling(1 / 255.0),\n",
    "        layers.Resizing(INPUT_SHAPE[0] + 20, INPUT_SHAPE[0] + 20),\n",
    "        layers.RandomCrop(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "    ],\n",
    "    name=\"train_data_augmentation\",\n",
    ")\n",
    "\n",
    "# Build the `val` and `test` data pipeline.\n",
    "test_aug = keras.Sequential(\n",
    "    [\n",
    "        layers.Rescaling(1 / 255.0),\n",
    "        layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    ],\n",
    "    name=\"test_data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "va4h0jW-Crze"
   },
   "outputs": [],
   "source": [
    "def train_map_fn(image, label):\n",
    "    return train_aug(image), label\n",
    "\n",
    "def test_map_fn(image, label):\n",
    "    return test_aug(image), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFJToGQFCo3i"
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_ds = (\n",
    "    train_ds.map(\n",
    "        train_map_fn, num_parallel_calls=AUTO\n",
    "    )\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_ds = (\n",
    "    val_ds.map(\n",
    "        test_map_fn, num_parallel_calls=AUTO\n",
    "    )\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_ds = (\n",
    "    test_ds.map(\n",
    "        test_map_fn, num_parallel_calls=AUTO\n",
    "    )\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2dZ0UnVlDGO4",
    "outputId": "555bca68-55d4-4116-b707-9a7a54c0e12f"
   },
   "outputs": [],
   "source": [
    "for image, label in train_ds.take(1):\n",
    "    print(image.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FpQDW0eDQwF"
   },
   "source": [
    "# PatchEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q9w8rUdFDMkP"
   },
   "outputs": [],
   "source": [
    "class PatchEmbed(layers.Layer):\n",
    "    \"\"\"Image patch embedding layer.\n",
    "\n",
    "    Args:\n",
    "        image_size (Tuple[int]): Input image resolution.\n",
    "        patch_size (Tuple[int]): Patch spatial resolution.\n",
    "        embed_dim (int): Embedding dimension.\n",
    "        add_pos_info (bool): Whether to add positional information to tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: Tuple[int] = (48, 48),\n",
    "        patch_size: Tuple[int] = (4, 4),\n",
    "        embed_dim: int = 32,\n",
    "        chunk_size: int = 8,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        patch_resolution = [\n",
    "            image_size[0] // patch_size[0],\n",
    "            image_size[1] // patch_size[1],\n",
    "        ]\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_resolution = patch_resolution\n",
    "        self.num_patches = patch_resolution[0] * patch_resolution[1]\n",
    "        self.proj = layers.Conv2D(\n",
    "            filters=embed_dim, kernel_size=patch_size, strides=patch_size\n",
    "        )\n",
    "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=self.num_patches, output_dim=embed_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        self.norm = keras.layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.chunking_layer = layers.Reshape(\n",
    "            target_shape=(self.num_patches//chunk_size, chunk_size, embed_dim)\n",
    "        )\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> Tuple[tf.Tensor, int, int, int]:\n",
    "        \"\"\"Patchifies the image, converts into tokens and adds pos information.\n",
    "\n",
    "        Args:\n",
    "            x: Tensor of shape (B, H, W, C)\n",
    "\n",
    "        Returns:\n",
    "            A tuple of the processed tensor, height of the projected\n",
    "            feature map, width of the projected feature map, number\n",
    "            of channels of the projected feature map.\n",
    "        \"\"\"\n",
    "        # Project the inputs.\n",
    "        x = self.proj(x)\n",
    "        x = self.flatten(x)\n",
    "        x = x + self.position_embedding(self.positions)\n",
    "\n",
    "        # B, H, W, C -> B, H*W, C\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Chunk the tokens in K\n",
    "        x = self.chunking_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApnyYrSTDnwy"
   },
   "source": [
    "# FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_ztuHnGDcAR"
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(layers.Layer):\n",
    "    \"\"\"Feed Forward Network.\n",
    "\n",
    "    Args:\n",
    "        dims (`int`): Dimension of the FFN.\n",
    "        dropout (`float`): Dropout probability of FFN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dims: int, dropout: float = 0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=4*dims, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=dims),\n",
    "                layers.Dropout(rate=dropout),\n",
    "            ]\n",
    "        )\n",
    "        self.add = layers.Add()\n",
    "        self.layernorm = layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "    def call(self, x: tf.Tensor):\n",
    "        x = self.add([x, self.ffn(x)])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVH2CUFDDq3d"
   },
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tap3IUcGDrZ1"
   },
   "outputs": [],
   "source": [
    "class Attention(layers.Layer):\n",
    "    \"\"\"The base attention module.\n",
    "    \n",
    "    Args:\n",
    "        num_heads (`int`): Number of attention heads.\n",
    "        key_dim (`int`): Size of each attention head for query and key.\n",
    "        dropout (`float`): Dropout probability for Attention Module.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, key_dim: int, dropout: float = 0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mha = layers.MultiHeadAttention(num_heads, key_dim, dropout=dropout)\n",
    "        self.layernorm = layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.add = layers.Add()\n",
    "\n",
    "    def call(self, query: tf.Tensor, key: tf.Tensor, value: tf.Tensor):\n",
    "        (attention_outs, attention_scores) = self.mha(\n",
    "            query=query,\n",
    "            key=key,\n",
    "            value=value,\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "        self.last_attention_scores = attention_scores\n",
    "        x = self.add([query, attention_outs])\n",
    "        x = self.layernorm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOH4DFJKDr5n"
   },
   "outputs": [],
   "source": [
    "class AttentionWithFFN(layers.Layer):\n",
    "    \"\"\"Self-attention module with FFN\n",
    "\n",
    "    Args:\n",
    "        ffn_dims (`int`): Number of units in FFN.\n",
    "        ffn_dropout (`float`): Dropout probability for FFN.\n",
    "        num_heads (`int`): Number of attention heads.\n",
    "        key_dim (`int`): Size of each attention head for query and key.\n",
    "        attn_dropout (`float`): Dropout probability for attention module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        ffn_dims: int = 128,\n",
    "        ffn_dropout: float = 0.1, \n",
    "        num_heads: int = 4,\n",
    "        key_dim: int = 256,\n",
    "        attn_dropout: float = 0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.attention = Attention(num_heads, key_dim, attn_dropout)\n",
    "        self.ffn = FeedForwardNetwork(ffn_dims, ffn_dropout)\n",
    "        \n",
    "    \n",
    "    def call(self, query: tf.Tensor, key: tf.Tensor, value: tf.Tensor):\n",
    "        x = self.attention(query, key, value)\n",
    "        x = self.ffn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFRetSjPD8Cx"
   },
   "source": [
    "# Recurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QT4kMDPnDuBP"
   },
   "outputs": [],
   "source": [
    "class CustomCell(layers.Layer):\n",
    "    \"\"\"Custom logic inside each recurrence.\n",
    "\n",
    "    Args:\n",
    "        chunk_size (`int`): Chunk size of the inputs.\n",
    "        r (`int`): One Cross Attention per **r** Self Attention.\n",
    "        num_layers (`int`): Number of layers in the Perceptual Model.\n",
    "        ffn_dims (`int`): Number of units in FFN.\n",
    "        ffn_dropout (`float`): Dropout probability for FFN.\n",
    "        num_heads (`int`): Number of attention heads.\n",
    "        key_dim (`int`): Size of each attention head for query and key.\n",
    "        attn_dropout (`float`): Dropout probability for attention module.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_size,\n",
    "        r=2,\n",
    "        num_layers: int = 5,\n",
    "        ffn_dims: int = 128,\n",
    "        ffn_dropout: float = 0.1, \n",
    "        num_heads: int = 4,\n",
    "        key_dim: int = 256,\n",
    "        attn_dropout: float = 0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.r = r\n",
    "        self.num_layers = num_layers\n",
    "        self.ffn_dims = ffn_dims\n",
    "        self.ffn_droput = ffn_dropout\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.attn_dropout = attn_dropout\n",
    "\n",
    "        self.state_size = tf.TensorShape([chunk_size, ffn_dims])\n",
    "        self.output_size = tf.TensorShape([chunk_size, ffn_dims])\n",
    "\n",
    "        ########################################################################\n",
    "        # Perceptual Module\n",
    "        ########################################################################\n",
    "        perceptual_module = list()\n",
    "        for layer_idx in range(num_layers):\n",
    "            perceptual_module.append(\n",
    "                AttentionWithFFN(\n",
    "                    ffn_dims=ffn_dims,\n",
    "                    ffn_dropout=ffn_dropout, \n",
    "                    num_heads=num_heads,\n",
    "                    key_dim=key_dim,\n",
    "                    attn_dropout=attn_dropout,\n",
    "                    name=f\"PM_SelfAttentionFFN{layer_idx}\")\n",
    "            )\n",
    "            if layer_idx % r == 0:\n",
    "                perceptual_module.append(\n",
    "                    AttentionWithFFN(\n",
    "                    ffn_dims=ffn_dims,\n",
    "                    ffn_dropout=ffn_dropout, \n",
    "                    num_heads=num_heads,\n",
    "                    key_dim=key_dim,\n",
    "                    attn_dropout=attn_dropout,\n",
    "                    name=f\"PM_CrossAttentionFFN{layer_idx}\")\n",
    "                )\n",
    "        self.perceptual_module = perceptual_module\n",
    "\n",
    "        ########################################################################\n",
    "        # Temporal Latent Bottleneck Module\n",
    "        ########################################################################\n",
    "        self.tlb_module = AttentionWithFFN(\n",
    "            ffn_dims=ffn_dims,\n",
    "            ffn_dropout=ffn_dropout, \n",
    "            num_heads=num_heads,\n",
    "            key_dim=key_dim,\n",
    "            attn_dropout=attn_dropout,\n",
    "            name=f\"TLBM_CrossAttentionFFN\"\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        # inputs => (batch, chunk_size, dims)\n",
    "        # states => [(batch, chunk_size, units)]\n",
    "\n",
    "        slow_stream = states[0]\n",
    "        fast_stream = inputs\n",
    "\n",
    "        for layer_idx, layer in enumerate(self.perceptual_module):\n",
    "            fast_stream = layer(\n",
    "                query=fast_stream,\n",
    "                key=fast_stream,\n",
    "                value=fast_stream\n",
    "            )\n",
    "            \n",
    "            if layer_idx % self.r == 0:\n",
    "                fast_stream = layer(\n",
    "                    query=fast_stream,\n",
    "                    key=slow_stream,\n",
    "                    value=slow_stream\n",
    "                )\n",
    "        \n",
    "        slow_stream = self.tlb_module(\n",
    "            query=slow_stream,\n",
    "            key=fast_stream,\n",
    "            value=fast_stream\n",
    "        )\n",
    "        \n",
    "        return fast_stream, [slow_stream]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z9EngzvgD99W",
    "outputId": "1cbf6100-6ef3-4bc1-ac22-aec2f37fe1cc"
   },
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "inputs = keras.Input((IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "# PATCH\n",
    "x = PatchEmbed(\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    patch_size=(PATCH_SIZE, PATCH_SIZE),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    ")(inputs)\n",
    "\n",
    "# RECURRENCE\n",
    "cell = CustomCell(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    r=R,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    ffn_dims=EMBED_DIM,\n",
    "    ffn_dropout=FFN_DROP, \n",
    "    num_heads=NUM_HEADS,\n",
    "    key_dim=EMBED_DIM,\n",
    "    attn_dropout=ATTN_DROP,\n",
    ")\n",
    "x = layers.RNN(cell)(x)\n",
    "outputs = tf.reduce_mean(x, axis=1)\n",
    "\n",
    "model = keras.models.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7pwBfVwFdPv",
    "outputId": "f1a67113-b3f8-4eb3-b579-d9ca6c7c2f3c"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Compile and train the model.\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtAaAc8zLhTi"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"accuracy\"], label=\"accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
