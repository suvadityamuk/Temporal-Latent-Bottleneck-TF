{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPS0hmz9tKyBpukY1Ufdkag"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "mcvLKPuQ1bZM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rGrN_j921V3L"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "LW2eH82X1fX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardNetwork(layers.Layer):\n",
        "    \"\"\"Feed Forward Network.\n",
        "\n",
        "    Args:\n",
        "        dims (`int`): Dimension of the FFN.\n",
        "        dropout (`float`): Dropout probability of FFN.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dims: int, dropout: float = 0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=4*dims, activation=tf.nn.gelu),\n",
        "                layers.Dense(units=dims),\n",
        "                layers.Dropout(rate=dropout),\n",
        "            ]\n",
        "        )\n",
        "        self.add = layers.Add()\n",
        "        self.layernorm = layers.LayerNormalization(epsilon=1e-5)\n",
        "\n",
        "    def call(self, x: tf.Tensor):\n",
        "        x = self.add([x, self.ffn(x)])\n",
        "        x = self.layernorm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5hxIRxgt4YY_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(layers.Layer):\n",
        "    \"\"\"The base attention module.\n",
        "    \n",
        "    Args:\n",
        "        num_heads (`int`): Number of attention heads.\n",
        "        key_dim (`int`): Size of each attention head for query and key.\n",
        "        dropout (`float`): Dropout probability for Attention Module.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads: int, key_dim: int, dropout: float = 0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.mha = layers.MultiHeadAttention(num_heads, key_dim, dropout=dropout)\n",
        "        self.layernorm = layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.add = layers.Add()\n",
        "\n",
        "    def call(self, query: tf.Tensor, key: tf.Tensor, value: tf.Tensor):\n",
        "        (attention_outs, attention_scores) = self.mha(\n",
        "            query=query,\n",
        "            key=key,\n",
        "            value=value,\n",
        "            return_attention_scores=True,\n",
        "        )\n",
        "        self.last_attention_scores = attention_scores\n",
        "        x = self.add([query, attention_outs])\n",
        "        x = self.layernorm(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "LkT_AnT84aGZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionWithFFN(layers.Layer):\n",
        "    \"\"\"Self-attention module with FFN\n",
        "\n",
        "    Args:\n",
        "        ffn_dims (`int`): Number of units in FFN.\n",
        "        ffn_dropout (`float`): Dropout probability for FFN.\n",
        "        num_heads (`int`): Number of attention heads.\n",
        "        key_dim (`int`): Size of each attention head for query and key.\n",
        "        attn_dropout (`float`): Dropout probability for attention module.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        ffn_dims: int = 128,\n",
        "        ffn_dropout: float = 0.1, \n",
        "        num_heads: int = 4,\n",
        "        key_dim: int = 256,\n",
        "        attn_dropout: float = 0.1,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attention = Attention(num_heads, key_dim, attn_dropout)\n",
        "        self.ffn = FeedForwardNetwork(ffn_dims, ffn_dropout)\n",
        "        \n",
        "    \n",
        "    def call(self, query: tf.Tensor, key: tf.Tensor, value: tf.Tensor):\n",
        "        x = self.attention(query, key, value)\n",
        "        x = self.ffn(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "BHTrQojQ1c1b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full model\n",
        "\n"
      ],
      "metadata": {
        "id": "3i7hV8nE1yDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCell(layers.Layer):\n",
        "    \"\"\"Custom logic inside each recurrence.\n",
        "\n",
        "    Args:\n",
        "        chunk_size (`int`): Chunk size of the inputs.\n",
        "        r (`int`): One Cross Attention per **r** Self Attention.\n",
        "        num_layers (`int`): Number of layers in the Perceptual Model.\n",
        "        ffn_dims (`int`): Number of units in FFN.\n",
        "        ffn_dropout (`float`): Dropout probability for FFN.\n",
        "        num_heads (`int`): Number of attention heads.\n",
        "        key_dim (`int`): Size of each attention head for query and key.\n",
        "        attn_dropout (`float`): Dropout probability for attention module.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        chunk_size,\n",
        "        r=2,\n",
        "        num_layers: int = 5,\n",
        "        ffn_dims: int = 128,\n",
        "        ffn_dropout: float = 0.1, \n",
        "        num_heads: int = 4,\n",
        "        key_dim: int = 256,\n",
        "        attn_dropout: float = 0.1,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.chunk_size = chunk_size\n",
        "        self.r = r\n",
        "        self.num_layers = num_layers\n",
        "        self.ffn_dims = ffn_dims\n",
        "        self.ffn_droput = ffn_dropout\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.attn_dropout = attn_dropout\n",
        "\n",
        "        self.state_size = tf.TensorShape([chunk_size, ffn_dims])\n",
        "        self.output_size = tf.TensorShape([chunk_size, ffn_dims])\n",
        "\n",
        "        ########################################################################\n",
        "        # Perceptual Module\n",
        "        ########################################################################\n",
        "        perceptual_module = list()\n",
        "        for layer_idx in range(num_layers):\n",
        "            perceptual_module.append(\n",
        "                AttentionWithFFN(\n",
        "                    ffn_dims=ffn_dims,\n",
        "                    ffn_dropout=ffn_dropout, \n",
        "                    num_heads=num_heads,\n",
        "                    key_dim=key_dim,\n",
        "                    attn_dropout=attn_dropout,\n",
        "                    name=f\"PM_SelfAttentionFFN{layer_idx}\")\n",
        "            )\n",
        "            if layer_idx % r == 0:\n",
        "                perceptual_module.append(\n",
        "                    AttentionWithFFN(\n",
        "                    ffn_dims=ffn_dims,\n",
        "                    ffn_dropout=ffn_dropout, \n",
        "                    num_heads=num_heads,\n",
        "                    key_dim=key_dim,\n",
        "                    attn_dropout=attn_dropout,\n",
        "                    name=f\"PM_CrossAttentionFFN{layer_idx}\")\n",
        "                )\n",
        "        self.perceptual_module = perceptual_module\n",
        "\n",
        "        ########################################################################\n",
        "        # Temporal Latent Bottleneck Module\n",
        "        ########################################################################\n",
        "        self.tlb_module = AttentionWithFFN(\n",
        "            ffn_dims=ffn_dims,\n",
        "            ffn_dropout=ffn_dropout, \n",
        "            num_heads=num_heads,\n",
        "            key_dim=key_dim,\n",
        "            attn_dropout=attn_dropout,\n",
        "            name=f\"TLBM_CrossAttentionFFN\"\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        # inputs => (batch, chunk_size, dims)\n",
        "        # states => [(batch, chunk_size, units)]\n",
        "\n",
        "        slow_stream = states[0]\n",
        "        fast_stream = inputs\n",
        "\n",
        "        for layer_idx, layer in enumerate(self.perceptual_module):\n",
        "            fast_stream = layer(\n",
        "                query=fast_stream,\n",
        "                key=fast_stream,\n",
        "                value=fast_stream\n",
        "            )\n",
        "            \n",
        "            if layer_idx % self.r == 0:\n",
        "                fast_stream = layer(\n",
        "                    query=fast_stream,\n",
        "                    key=slow_stream,\n",
        "                    value=slow_stream\n",
        "                )\n",
        "        \n",
        "        slow_stream = self.tlb_module(\n",
        "            query=slow_stream,\n",
        "            key=fast_stream,\n",
        "            value=fast_stream\n",
        "        )\n",
        "        \n",
        "        return fast_stream, [slow_stream]"
      ],
      "metadata": {
        "id": "dIjRnliV19hR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the model"
      ],
      "metadata": {
        "id": "UNZxVkGYAlTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.clear_session()\n",
        "\n",
        "units = 32\n",
        "chunk_size = 8\n",
        "\n",
        "dims = 32\n",
        "\n",
        "batch_size = 64\n",
        "num_batches = 10\n",
        "timestep = 80\n",
        "\n",
        "inputs = tf.random.normal(\n",
        "    (batch_size, timestep//chunk_size, chunk_size, dims)\n",
        ")\n",
        "\n",
        "cell = CustomCell(\n",
        "    chunk_size=8,\n",
        "    r=2,\n",
        "    num_layers=5,\n",
        "    ffn_dims=32,\n",
        "    ffn_dropout=0.1, \n",
        "    num_heads=4,\n",
        "    key_dim=32,\n",
        "    attn_dropout=0.1,\n",
        ")\n",
        "rnn = layers.RNN(cell, return_sequences=True)\n",
        "rnn(inputs).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tng8JasuAhdN",
        "outputId": "5fd70346-7096-471c-e274-831071bf4719"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 10, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}