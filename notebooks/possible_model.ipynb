{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m layers\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(layers.Layer):\n",
    "    \"\"\"Feed Forward Network.\n",
    "\n",
    "    Args:\n",
    "        dims (`int`): Dimension of the model.\n",
    "        dropout (`float`): Dropout probability.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dims: int, dropout: float = 0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(units=4*dims, activation=tf.nn.gelu),\n",
    "                layers.Dense(units=dims),\n",
    "                layers.Dropout(rate=dropout),\n",
    "            ]\n",
    "        )\n",
    "        self.add = layers.Add()\n",
    "        self.layernorm = layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "    def call(self, x: tf.Tensor):\n",
    "        x = self.add([x, self.ffn(x)])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BaseAttention(layers.Layer):\n",
    "    \"\"\"The base attention module.\n",
    "    \n",
    "    Args:\n",
    "        num_heads (`int`): Number of attention heads.\n",
    "        key_dim (`int`): Size of each attention head for query and key.\n",
    "        dropout (`float`): Dropout probability.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads: int, key_dim: int, dropout: float = 0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mha = layers.MultiHeadAttention(num_heads, key_dim, dropout=dropout)\n",
    "        self.layernorm = layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.add = layers.Add()\n",
    "\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x: tf.Tensor, context: tf.Tensor):\n",
    "        (attention_outs, attention_scores) = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "        self.last_attention_scores = attention_scores\n",
    "        x = self.add([x, attention_outs])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SelfAttention(BaseAttention):\n",
    "    def call(self, x):\n",
    "        (attention_outputs, attention_scores) = self.mha(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "        self.last_attention_scores = attention_scores\n",
    "        x = self.add([x, attention_outputs])\n",
    "        x = self.layernorm(x)\n",
    "        return x\n",
    "\n",
    "class SelfAttentionWithFFN(layers.Layer):\n",
    "    \"\"\"Self-attention module with FFN\n",
    "\n",
    "    Args:\n",
    "        ffn_dims (`int`): Number of units in FFN\n",
    "        ffn_dropout (`float`): Dropout probability for FFN\n",
    "        num_heads (`int`): Number of attention heads\n",
    "        key_dim (`int`): Size of each attention head for query and key.\n",
    "        dropout (`float`): Dropout probability.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, ffn_dims: int = 128, ffn_dropout: float = 0.1, \n",
    "                 num_heads: int = 4, key_dim: int = 256, dropout: float = 0.1, **kwargs):\n",
    "        self.self_attention = SelfAttention(num_heads, key_dim, dropout, **kwargs)\n",
    "        self.ffn = FeedForwardNetwork(ffn_dims, ffn_dropout)\n",
    "    \n",
    "    def call(self, x):\n",
    "        attention_outputs = self.self_attention(x)\n",
    "        x = self.ffn(attention_outputs)\n",
    "        return x\n",
    "\n",
    "class CrossAttentionWithFFN(layers.Layer):\n",
    "    \"\"\"Cross-attention module with FFN\n",
    "\n",
    "    Args:\n",
    "        ffn_dims (`int`): Number of units in FFN\n",
    "        ffn_dropout (`float`): Dropout probability for FFN\n",
    "        num_heads (`int`): Number of attention heads\n",
    "        key_dim (`int`): Size of each attention head for query and key.\n",
    "        dropout (`float`): Dropout probability.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, ffn_dims: int = 128, ffn_dropout: float = 0.1, \n",
    "                 num_heads: int = 4, key_dim: int = 256, dropout: float = 0.1, **kwargs):\n",
    "        self.cross_attention = CrossAttention(num_heads, key_dim, dropout, **kwargs)\n",
    "        self.ffn = FeedForwardNetwork(ffn_dims, ffn_dropout)\n",
    "    \n",
    "    def call(self, x, context):\n",
    "        attention_outputs = self.cross_attention(x, context)\n",
    "        x = self.ffn(attention_outputs)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough implementation of full model\n",
    "\n",
    "As I can understand from the custom RNN, a large doubt of mine on how I could handle the size of the batch and chunks within the init function scope was solved, the implementation was more or less straightforward, but makes a few assumptions and has severe blocking overheads we need to try and optimize\n",
    "\n",
    "- Interesting question to ask: When initializing, do we take `num_layers` as the limiting factor, or `R` as the limiting factor?\n",
    "Example: If we have `num_layers = 5` and `R = 2`, to maintain the last layer as Self-Attention, we'd have to exceed `num_layers`.\n",
    "On the other hand, if we kept num_layers as limiting, we'd have a Cross Attention layer as the last one. \n",
    "The implementation below puts `R` as the limiting factor.\n",
    "\n",
    "- There is a O(n^2) loop inside the Perceptual Module initialization, which became necessary to create the dynamic number of layers according to the restrictions of `num_layers` and `R`. Any way to optimize/vectorize that one?\n",
    "\n",
    "- There is a really bad-looking loop in the `call` function that is necessary to identify the SelfAttention and CrossAttention layers and then accordingly give them an input. Any solutions for that?\n",
    "\n",
    "- Obviously, need to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCell(layers.Layer):\n",
    "    def __init__(self, chunk_size, r, num_layers: int = 5, ffn_dims: int = 128, ffn_dropout: float = 0.1, \n",
    "                 num_heads: int = 4, key_dim: int = 256, dropout: float = 0.1, **kwargs):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.r = r\n",
    "        self.num_layers = num_layers\n",
    "        self.ffn_dims = ffn_dims\n",
    "        self.ffn_droput = ffn_dropout\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.state_size = tf.TensorShape([chunk_size, ffn_dims])\n",
    "        self.output_size = tf.TensorShape([chunk_size, ffn_dims])\n",
    "\n",
    "        # This is the point where we need to add our custom logic\n",
    "        # instead of the MLP\n",
    "\n",
    "        # Update: Added some possible logic to mimic the Perceptual Module behavior, although testing is needed\n",
    "\n",
    "        # Perceptual Module\n",
    "        perceptual_module_layers = []\n",
    "        while len(perceptual_module_layers) <= num_layers:\n",
    "            layers.append(SelfAttentionWithFFN(ffn_dims, ffn_dropout, num_heads, key_dim, dropout))\n",
    "            if len(perceptual_module_layers) == num_layers:\n",
    "                break\n",
    "            else:\n",
    "                self_attention_layers = [SelfAttentionWithFFN(ffn_dims, ffn_dropout, num_heads, key_dim, dropout) for _ in range(r-1)]\n",
    "                if len(self_attention_layers) != 0:\n",
    "                    perceptual_module_layers.append(*self_attention_layers)\n",
    "                perceptual_module_layers.append(CrossAttentionWithFFN(ffn_dims, ffn_dropout, num_heads, key_dim, dropout))\n",
    "    \n",
    "        self.perceptual_module = perceptual_module_layers\n",
    "\n",
    "        # Update: Added some possible logic to mimic the TLB module behavior, although testing is again needed\n",
    "\n",
    "        # Temporal Latent Bottleneck\n",
    "        self.tlb_cross_attention = CrossAttentionWithFFN(ffn_dims, ffn_dropout, num_heads, key_dim, dropout)\n",
    "\n",
    "\n",
    "        # self.mlp = keras.Sequential([\n",
    "        #     layers.Dense(units, activation=\"relu\"),\n",
    "        #     layers.Dense(units * 4, activation=\"relu\"),\n",
    "        #     layers.Dense(units, activation=\"relu\"),\n",
    "        # ])\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def possible_implemented_call(self, inputs, states):\n",
    "        slow_stream = states[0]\n",
    "        \n",
    "        fast_stream = inputs # chunks\n",
    "        for layer in self.perceptual_module:\n",
    "            if isinstance(layer, SelfAttentionWithFFN):\n",
    "                fast_stream = layer(fast_stream, fast_stream, fast_stream)\n",
    "            elif isinstance(layer, CrossAttentionWithFFN):\n",
    "                fast_stream = layer(fast_stream, slow_stream, slow_stream)\n",
    "\n",
    "        slow_stream = self.tlb_cross_attention(slow_stream, fast_stream, fast_stream)\n",
    "\n",
    "        return fast_stream, [slow_stream]\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        # inputs => (batch, chunk_size, dims)\n",
    "        # states => [(batch, chunk_size, units)]\n",
    "\n",
    "        prev_state = states[0]\n",
    "\n",
    "        outputs = self.mlp(inputs)\n",
    "\n",
    "        new_state = outputs + prev_state\n",
    "        \n",
    "        return outputs, [new_state]\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"units\": self.units, \"chunk_size\": self.chunk_size}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
