{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from typing import Tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eun5qTwX9rKj"
      },
      "outputs": [],
      "source": [
        "# DATA\n",
        "TRAIN_SLICE = 40000\n",
        "BUFFER_SIZE = 2048\n",
        "BATCH_SIZE = 1024\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "INPUT_SHAPE = (32, 32, 3)\n",
        "IMAGE_SIZE = 48\n",
        "NUM_CLASSES = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "82fe1NVY9es_"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "(x_train, y_train), (x_val, y_val) = (\n",
        "    (x_train[:TRAIN_SLICE], y_train[:TRAIN_SLICE]),\n",
        "    (x_train[TRAIN_SLICE:], y_train[TRAIN_SLICE:]),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xesHgE-L9iLD"
      },
      "outputs": [],
      "source": [
        "# Build the `train` augmentation pipeline.\n",
        "train_aug = keras.Sequential(\n",
        "    [\n",
        "        layers.Rescaling(1 / 255.0),\n",
        "        layers.Resizing(INPUT_SHAPE[0] + 20, INPUT_SHAPE[0] + 20),\n",
        "        layers.RandomCrop(IMAGE_SIZE, IMAGE_SIZE),\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "    ],\n",
        "    name=\"train_data_augmentation\",\n",
        ")\n",
        "\n",
        "# Build the `val` and `test` data pipeline.\n",
        "test_aug = keras.Sequential(\n",
        "    [\n",
        "        layers.Rescaling(1 / 255.0),\n",
        "        layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    ],\n",
        "    name=\"test_data_augmentation\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "worauXu39n89",
        "outputId": "43f21dce-a8f8-4bce-f955-5c7c1817a9af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ],
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_ds = (\n",
        "    train_ds.map(\n",
        "        lambda image, label: (train_aug(image), label), num_parallel_calls=AUTO\n",
        "    )\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_ds = (\n",
        "    val_ds.map(lambda image, label: (test_aug(image), label), num_parallel_calls=AUTO)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_ds = (\n",
        "    test_ds.map(lambda image, label: (test_aug(image), label), num_parallel_calls=AUTO)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC5-tbhG9wLk",
        "outputId": "0b999075-a712-4c8a-9c7b-f25906a63382"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1024, 48, 48, 3)\n",
            "(1024, 1)\n"
          ]
        }
      ],
      "source": [
        "for image, label in train_ds.take(1):\n",
        "    print(image.shape)\n",
        "    print(label.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rla1kn1R92KS"
      },
      "outputs": [],
      "source": [
        "class PatchEmbed(layers.Layer):\n",
        "    \"\"\"Image patch embedding layer.\n",
        "\n",
        "    Args:\n",
        "        image_size (Tuple[int]): Input image resolution.\n",
        "        patch_size (Tuple[int]): Patch spatial resolution.\n",
        "        embed_dim (int): Embedding dimension.\n",
        "        add_pos_info (bool): Whether to add positional information to tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image_size: Tuple[int] = (224, 224),\n",
        "        patch_size: Tuple[int] = (4, 4),\n",
        "        embed_dim: int = 96,\n",
        "        chunk_size: int = 8,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        patch_resolution = [\n",
        "            image_size[0] // patch_size[0],\n",
        "            image_size[1] // patch_size[1],\n",
        "        ]\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.patch_resolution = patch_resolution\n",
        "        self.num_patches = patch_resolution[0] * patch_resolution[1]\n",
        "        self.proj = layers.Conv2D(\n",
        "            filters=embed_dim, kernel_size=patch_size, strides=patch_size\n",
        "        )\n",
        "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=self.num_patches, output_dim=embed_dim\n",
        "        )\n",
        "        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        self.norm = keras.layers.LayerNormalization(epsilon=1e-7)\n",
        "        self.chunking_layer = layers.Reshape(\n",
        "            target_shape=(self.num_patches//chunk_size, chunk_size, embed_dim)\n",
        "        )\n",
        "\n",
        "    def call(self, x: tf.Tensor) -> Tuple[tf.Tensor, int, int, int]:\n",
        "        \"\"\"Patchifies the image, converts into tokens and adds pos information.\n",
        "\n",
        "        Args:\n",
        "            x: Tensor of shape (B, H, W, C)\n",
        "\n",
        "        Returns:\n",
        "            A tuple of the processed tensor, height of the projected\n",
        "            feature map, width of the projected feature map, number\n",
        "            of channels of the projected feature map.\n",
        "        \"\"\"\n",
        "        # Project the inputs.\n",
        "        x = self.proj(x)\n",
        "\n",
        "        # Obtain the shape from the projected tensor.\n",
        "        height = tf.shape(x)[1]\n",
        "        width = tf.shape(x)[2]\n",
        "        channels = tf.shape(x)[3]\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = x + self.position_embedding(self.positions)\n",
        "\n",
        "        # B, H, W, C -> B, H*W, C\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Chunk the tokens in K\n",
        "        x = self.chunking_layer(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44p_C78D-9e5",
        "outputId": "a8c977c5-7f49-46cc-ab05-8950b04af996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1024, 48, 48, 3)\n",
            "(1024, 8, 8, 32)\n"
          ]
        }
      ],
      "source": [
        "patch_embed = PatchEmbed(\n",
        "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    patch_size=(6, 6),\n",
        "    embed_dim=32,\n",
        ")\n",
        "\n",
        "for image, label in train_ds.take(1):\n",
        "    print(image.shape)\n",
        "    print(patch_embed(image).shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "scratchpad",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.1"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
