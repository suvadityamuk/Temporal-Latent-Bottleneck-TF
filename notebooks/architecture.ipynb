{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkwpaxcJpkxa"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDhDHOmnpmFA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9cyhJ-aon6m"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODl3BH9zopTr"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNetwork(layers.Layer):\n",
        "    \"\"\"Feed Forward Network.\n",
        "\n",
        "    Args:\n",
        "        dims (`int`): Dimension of the model.\n",
        "        dropout (`float`): Dropout probability.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dims: int, dropout: float = 0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=4*dims, activation=tf.nn.gelu),\n",
        "                layers.Dense(units=dims),\n",
        "                layers.Dropout(rate=dropout),\n",
        "            ]\n",
        "        )\n",
        "        self.add = layers.Add()\n",
        "        self.layernorm = layers.LayerNormalization(epsilon=1e-5)\n",
        "\n",
        "    def call(self, x: tf.Tensor):\n",
        "        x = self.add([x, self.ffn(x)])\n",
        "        x = self.layernorm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BaseAttention(layers.Layer):\n",
        "    \"\"\"The base attention module.\n",
        "    \n",
        "    Args:\n",
        "        num_heads (`int`): Number of attention heads.\n",
        "        key_dim (`int`): Size of each attention head for query and key.\n",
        "        dropout (`float`): Dropout probability.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads: int, key_dim: int, dropout: float = 0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.mha = layers.MultiHeadAttention(num_heads, key_dim, dropout=dropout)\n",
        "        self.layernorm = layers.LayerNormalization(epsilon=1e-5)\n",
        "        self.add = layers.Add()\n",
        "\n",
        "\n",
        "class CrossAttention(BaseAttention):\n",
        "    def call(self, x: tf.Tensor, context: tf.Tensor):\n",
        "        (attention_outs, attention_scores) = self.mha(\n",
        "            query=x,\n",
        "            key=context,\n",
        "            value=context,\n",
        "            return_attention_scores=True,\n",
        "        )\n",
        "        self.last_attention_scores = attention_scores\n",
        "        x = self.add([x, attention_outs])\n",
        "        x = self.layernorm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SelfAttention(BaseAttention):\n",
        "    def call(self, x):\n",
        "        (attention_outputs, attention_scores) = self.mha(\n",
        "            query=x,\n",
        "            key=x,\n",
        "            value=x,\n",
        "            return_attention_scores=True,\n",
        "        )\n",
        "        self.last_attention_scores = attention_scores\n",
        "        x = self.add([x, attention_outputs])\n",
        "        x = self.layernorm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic implementation - No super class inheritance below >>\n",
        "\n",
        "class SelfAttentionWithFFN(layers.Layer):\n",
        "    \"\"\"Self-attention module with FFN\n",
        "\n",
        "    Args:\n",
        "        ffn_dims (`int`): Number of units in FFN\n",
        "        ffn_dropout (`float`): Dropout probability for FFN\n",
        "        num_heads (`int`): Number of attention heads\n",
        "        key_dim (`int`): Size of each attention head for query and key.\n",
        "        dropout (`float`): Dropout probability.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, ffn_dims: int = 128, ffn_dropout: float = 0.1, \n",
        "                 num_heads: int = 4, key_dim: int = 256, dropout: float = 0.1, **kwargs):\n",
        "        self.self_attention = SelfAttention(num_heads, key_dim, dropout, **kwargs)\n",
        "        self.ffn = FeedForwardNetwork(ffn_dims, ffn_dropout)\n",
        "    \n",
        "    def call(self, x):\n",
        "        attention_outputs = self.self_attention(x)\n",
        "        x = self.ffn(attention_outputs)\n",
        "        return x\n",
        "\n",
        "class CrossAttentionWithFFN(layers.Layer):\n",
        "    \"\"\"Cross-attention module with FFN\n",
        "\n",
        "    Args:\n",
        "        ffn_dims (`int`): Number of units in FFN\n",
        "        ffn_dropout (`float`): Dropout probability for FFN\n",
        "        num_heads (`int`): Number of attention heads\n",
        "        key_dim (`int`): Size of each attention head for query and key.\n",
        "        dropout (`float`): Dropout probability.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, ffn_dims: int = 128, ffn_dropout: float = 0.1, \n",
        "                 num_heads: int = 4, key_dim: int = 256, dropout: float = 0.1, **kwargs):\n",
        "        self.cross_attention = CrossAttention(num_heads, key_dim, dropout, **kwargs)\n",
        "        self.ffn = FeedForwardNetwork(ffn_dims, ffn_dropout)\n",
        "    \n",
        "    def call(self, x, context):\n",
        "        attention_outputs = self.cross_attention(x, context)\n",
        "        x = self.ffn(attention_outputs)\n",
        "        return x\n",
        "\n",
        "# Nuanced implementation - Super class inheritance performed below >>\n",
        "\n",
        "# class SelfAttentionWithFFN(SelfAttention):\n",
        "#     def __init__(self, ffn_dims: int = 128, ffn_dropout: float = 0.1, \n",
        "#           num_heads: int = 4, key_dim: int = 256, dropout: float = 0.1, **kwargs):\n",
        "#         super(SelfAttentionWithFFN, self).__init__(num_heads, key_dim, dropout, **kwargs)\n",
        "#         self.ffn = FeedForwardNetwork(ffn_dims, ffn_dropout)\n",
        "\n",
        "#     def call(self, x):\n",
        "#         x = super.call(x)\n",
        "#         x = self.ffn(x)\n",
        "#         return x\n",
        "\n",
        "# class CrossAttentionWithFFN(CrossAttention):\n",
        "#     def __init__(self, ffn_dims: int = 128, ffn_dropout: float = 0.1, \n",
        "#                  num_heads: int = 4, key_dim: int = 256, dropout: float = 0.1, **kwargs):\n",
        "#         super(CrossAttentionWithFFN, self).__init__(num_heads, key_dim, dropout, **kwargs)\n",
        "#         self.ffn = FeedForwardNetwork(ffn_dims, ffn_dropout)\n",
        "    \n",
        "#     def call(self, x, context):\n",
        "#         x = super.call(x, context)\n",
        "#         x = self.ffn(x)\n",
        "#         return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PD8IhdZ5ohI-"
      },
      "source": [
        "# Perceptual Module (WIP - sharp edges)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PerceptualModule(layers.Layer):\n",
        "    def __init__(self, ffn_dims: int = 128, ffn_dropout: float = 0.1, \n",
        "                 num_heads: int = 4, key_dim: int = 256, dropout: float=0.1, **kwargs):\n",
        "        self.sa_ffn1 = SelfAttentionWithFFN(ffn_dims, ffn_dropout, num_heads, key_dim, dropout)\n",
        "        self.ca_ffn1 = CrossAttentionWithFFN(ffn_dims, ffn_dropout, num_heads, key_dim, dropout)\n",
        "        self.sa_ffn2 = SelfAttentionWithFFN(ffn_dims, ffn_dropout, num_heads, key_dim, dropout)\n",
        "        self.ca_ffn2 = CrossAttentionWithFFN(ffn_dims, ffn_dropout, num_heads, key_dim, dropout)\n",
        "        self.sa_ffn3 = SelfAttentionWithFFN(ffn_dims, ffn_dropout, num_heads, key_dim, dropout)\n",
        "\n",
        "        # Need to supply a `state_size` and `output_size` to the RNN Layer from within the cell \n",
        "        # to indicate what sizes it should allocate for intermediate stages\n",
        "        self.state_size = None\n",
        "        self.output_size = None\n",
        "\n",
        "    def call(self, x: tf.Tensor, slow_stream_context: tf.Tensor):\n",
        "        x = self.sa_ffn1(x)\n",
        "        x = self.ca_ffn1(x, slow_stream_context)\n",
        "        x = self.sa_ffn2(x)\n",
        "        x = self.ca_ffn2(x, slow_stream_context)\n",
        "        x = self.sa_ffn3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBqNQOs7ojMe"
      },
      "source": [
        "# Temporal Latent Bottleneck Model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNKcguMbcPs6iN6jqn/By7F",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
